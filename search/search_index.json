{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Space-map","text":"<p>Reconstructing atlas-level single-cell 3D tissue maps from serial sections</p>"},{"location":"#about-spacemap","title":"About SpaceMap","text":"<p>SpaceMap is an open-source framework for reconstructing atlas-level single-cell 3D tissue maps from serial sections. It integrates single-cell coordinates with optional histological image features to assemble consecutive tissue sections into coherent 3D models, combining multi-scale feature matching with large-deformation diffeomorphic metric mapping (LDDMM) to deliver global reconstructions while preserving local micro-anatomy.</p> <p>High\u2011resolution three\u2011dimensional (3D) tissue atlases are transforming how we study cellular architecture\u2011function relationships in human tissues. However, an accurate and efficient reconstruction method that can handle atlas-scale datasets remains elusive. SpaceMap addresses this challenge by combining multi\u2011scale feature matching with large\u2011deformation diffeomorphic metric mapping, delivering global reconstructions while preserving local micro\u2011anatomy.</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-modal Registration: Combines cell coordinates, cell types, gene expression, and histological images for robust alignment</li> <li>Two-stage Registration Approach: Efficient coarse alignment followed by precise fine registration</li> <li>Advanced Feature Matching: Combines deep learning (LoFTR) with traditional computer vision methods (SIFT)</li> <li>GPU-accelerated LDDMM: Optimized for handling large-scale cellular data from multiple tissue sections</li> <li>Global Consistency: Ensures structural coherence between non-adjacent sections</li> <li>High Performance: ~2-fold more accurate than PASTE and STalign while running on standard laptop hardware</li> <li>Scalability: Designed for atlas-scale datasets with millions of cells</li> <li>Preservation of Structure: Maintains micro-anatomical structures through diffeomorphic mapping</li> <li>Flexibility: Works with various spatial omics technologies (Xenium, CODEX, etc.)</li> <li>Visualization: Built-in tools for exploring and validating 3D reconstructions</li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>SpaceMap has been successfully applied to build high-resolution 3D tissue maps of:</p> <ul> <li>Serial sectioned spatial transcriptomics (Xenium, ~2.9M cells)</li> <li>Spatial proteomics dataset (CODEX, ~2.4M cells)</li> <li>3D models for diseased (colon polyp) and reference colon</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import spacemap\nfrom spacemap import Slice\nimport pandas as pd\n\n# Load cell coordinate data from CSV\ndf = pd.read_csv(\"cells.csv.gz\")\ngroups = df.groupby(\"layer\")\n\n# Organize data by layers\nxys = []  # xy coordinates for each layer\nids = []  # layer IDs\n\nfor layer, dff in groups:\n    xy = dff[[\"x\", \"y\"]].values\n    ids.append(layer)\n    xys.append(xy)\n\n# Set up project\nbase = \"data/flow\"\nflow = spacemap.flow.FlowImport(base)\nflow.init_xys(xys, ids)\nslices = flow.slices\n\n# Perform affine registration for coarse alignment\nmgr = spacemap.flow.AutoFlowMultiCenter3(slices)\nmgr.alignMethod = \"auto\"\nmgr.affine(\"DF\", show=True)\n\n# Perform LDDMM for precise alignment\nmgr.ldm_pair(Slice.align1Key, Slice.align2Key, show=True)\n\n# Export results\nexport = spacemap.flow.FlowExport(slices)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install spacemap\n</code></pre> <p>For detailed installation instructions, see the Installation Guide.</p>"},{"location":"#methodology","title":"Methodology","text":"<p>SpaceMap employs a two-stage registration approach:</p> <ol> <li>Coarse Alignment: Applies affine transformations for adjacent sections, computationally efficient and avoids local optima</li> <li>Fine Mapping: Focuses exclusively on local non-linear deformations using GPU-accelerated LDDMM</li> </ol> <p>For more details on the methodology, refer to our paper: [Citation information]</p> <p></p>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>The SpaceMap framework is organized around several interconnected systems that handle different aspects of the 3D reconstruction process. The system is centered on core registration capabilities (affine and LDDMM), supported by data management, feature matching, and visualization tools.</p>"},{"location":"#system-architecture-diagram","title":"System Architecture Diagram","text":"<p>[System architecture diagram will be added here]</p>"},{"location":"#registration-pipeline-technical-approach","title":"Registration Pipeline &amp; Technical Approach","text":"<p>SpaceMap employs a multi-stage registration pipeline to achieve accurate alignment of tissue sections:</p> <ol> <li>Coarse Alignment: Applies affine transformations between adjacent sections. This step is computationally efficient, handles global positioning and orientation, and avoids local optima that can trap non-linear methods.</li> <li>Fine Mapping: Uses Large Deformation Diffeomorphic Metric Mapping (LDDMM) for local non-linear deformations, preserving micro-anatomical structures. This step is GPU-accelerated for high performance.</li> </ol> <p>This two-stage approach allows SpaceMap to efficiently handle large datasets while capturing complex local deformations necessary for accurate tissue reconstruction.</p> <p>Multi-Resolution Processing: - The registration process starts with coarse alignment, followed by feature matching refinement and fine alignment using LDDMM. - Parallel processing is used to handle multiple slice pairs efficiently, further improving scalability.</p>"},{"location":"#registration-process-flow","title":"Registration Process Flow","text":"<p>[Registration process flow diagram will be added here]</p>"},{"location":"#key-components","title":"Key Components","text":"Class Module Purpose <code>FlowImport</code> flow Handles data import and initialization of slice data <code>Slice</code> base.flowBase Manages individual tissue sections and their transformations <code>AutoFlowMultiCenter</code> flow Orchestrates the registration workflow <code>LDDMMRegistration</code> registration Performs non-linear deformable registration <code>TransformDB</code> flow.outputs Stores and manages transformations between sections <code>FlowExport</code> flow Handles exporting of aligned data and 3D models <code>find_matches</code> matches Identifies corresponding features between sections <code>affine_register</code> affine Performs affine registration for coarse alignment"},{"location":"#component-relationships","title":"Component Relationships","text":"<p>[Component relationships diagram will be added here]</p>"},{"location":"#summary-of-capabilities","title":"Summary of Capabilities","text":"<p>SpaceMap offers several advantages for 3D tissue reconstruction:</p> <ol> <li>Integration of Multiple Data Types: Combines cell coordinates, histology images, and other spatial data</li> <li>Scalability: Designed for atlas-scale datasets with millions of cells</li> <li>Preservation of Structure: Maintains micro-anatomical structures through diffeomorphic mapping</li> <li>Efficiency: Two-stage registration with parallel processing optimizes computational resources</li> <li>Flexibility: Works with various spatial omics technologies (Xenium, CODEX, etc.)</li> <li>Visualization: Built-in tools for exploring and validating 3D reconstructions</li> </ol> <p>These capabilities make SpaceMap suitable for creating comprehensive 3D models from serial tissue sections across a wide range of biological applications, from basic research to clinical studies.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Rongduo Han<sup>1,2,*</sup> - Implementation, Writing</li> <li>Chenchen Zhu<sup>3,*</sup> - Conceptualization, Data Analysis, Writing</li> <li>Cihan Ruan<sup>2,*</sup> - Conceptualization, Writing</li> <li>Bingqing Zhao<sup>3</sup> - Xenium Experiment</li> <li>Yuqi Tan<sup>4</sup> - CODEX Data Processing</li> <li>Emma Monte<sup>2</sup> - Project Management</li> <li>Bei Wei<sup>5,2</sup> - CODEX Experiment</li> <li>Joanna Bi<sup>2</sup> - Xenium and CODEX Experiments</li> <li>Thomas V. Karathanos<sup>3</sup></li> <li>Rozelle Laquindanum</li> <li>Greg Charville<sup>4</sup></li> <li>Meng Wang<sup>6</sup></li> <li>Yiing Lin<sup>3</sup></li> <li>James M. Ford<sup>3</sup></li> <li>Garry Nolan<sup>3</sup></li> <li>Nam Ling<sup>2</sup></li> <li>Michael Snyder<sup>3,#</sup> - Conceptualization, Writing</li> </ul> <p><sup>1</sup> College of Software, Nankai University, Tianjin, China <sup>2</sup> Department of Computer Science and Engineering, Santa Clara University, Santa Clara, CA, USA <sup>3</sup> Department of Genetics, Stanford School of Medicine, Stanford, CA 94305 <sup>4</sup> Department of Pathology, Stanford School of Medicine, Stanford, CA 94305 <sup>5</sup> Sun Yat-Sen Memorial Hospital, Sun Yat-Sen University, Guangzhou, Guangdong, P.R.China <sup>6</sup> Michigan University  </p> <p><sup>*</sup> These authors contributed equally to this study <sup>#</sup> Correspondence to: mpsnyder@stanford.edu</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>We gratefully thank: - Pauline Chu from Stanford Human Pathology/Histology Service Center for sectioning tissues and preparing slides - Shubham/Shannon for colectomy - Teri, Jeanne, Zhengyan (Yiing's lab), Uri, and Ed for their contributions</p>"},{"location":"#funding","title":"Funding","text":"<p>This work was funded by: - NIH Common Fund HuBMAP program (Phase 1 U54HG010426 and Phase 2 U54HG012723) - NCI HTAN program (U2CCA233311) - HuBMAP JumpStart Fellowship (3OT2OD033759-01S3) - AWS Cloud Credit for Research</p> <p>The funding sources had no role in the design of the study, data analysis, or preparation of the manuscript.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use SpaceMap in your research, please cite our paper:</p> <pre><code>[Citation details]\n</code></pre>"},{"location":"alignment/image-alignment/","title":"Image Alignment","text":"<p>Related source files: - affine/AutoScale.py - affine/BestRotate.py - affine/FilterGlobal.py - affine/FinalRotate.py - affine/ManualRotate.py - affine_block/AutoAffineImgKey.py - affine_block/FilterGlobalImg.py - affine_block/ManualRotateImg.py - find/dice4.py - flow/init.py - flow/afFlow2MultiDF.py</p>"},{"location":"alignment/image-alignment/#introduction","title":"Introduction","text":"<p>The image alignment system in SpaceMap provides a multi-stage registration workflow for aligning consecutive tissue sections, enabling high-precision 3D reconstruction. This workflow combines affine registration (coarse alignment) and LDDMM (Large Deformation Diffeomorphic Metric Mapping, fine nonlinear deformation), balancing efficiency and accuracy.</p> <ul> <li>For 3D reconstruction details, see: 3D Reconstruction</li> <li>For LDDMM registration system, see: Registration System (LDDMM)</li> </ul>"},{"location":"alignment/image-alignment/#alignment-pipeline-overview","title":"Alignment Pipeline Overview","text":"<p>The registration workflow adopts a hierarchical coarse-to-fine strategy to optimize both efficiency and accuracy:</p> <ol> <li>Preprocessing and normalization</li> <li>Coarse registration (affine)</li> <li>Fine registration (LDDMM)</li> <li>Multi-center registration strategy</li> <li>Transformation management and composition</li> <li>Results used for 3D reconstruction</li> </ol>"},{"location":"alignment/image-alignment/#preprocessing-and-normalization","title":"Preprocessing and Normalization","text":"<p>Before registration, images need to be normalized for feature matching: - Downsample to detect global intensity trends - Calculate nonzero mean and normalize intensity - Ensure consistent data format for subsequent processing</p> <p>Related method: <code>AutoAffineImgKey.process_init</code></p>"},{"location":"alignment/image-alignment/#coarse-registration-autoaffineimgkey","title":"Coarse Registration (AutoAffineImgKey)","text":"<ul> <li>Feature point matching (SIFT/LOFTR, auto selection)</li> <li>Match filtering (FilterGraphImg/FilterGlobalImg)</li> <li>Multi-resolution affine optimization (LDMAffine, AutoGradImg)</li> </ul> <p>Related code snippets: - AutoAffineImgKey.py#L6-L68 - FilterGlobalImg.py#L6-L51</p>"},{"location":"alignment/image-alignment/#fine-registration-lddmm","title":"Fine Registration (LDDMM)","text":"<ul> <li>Input coarse registration results</li> <li>Initialize LDDMMRegistration, set GPU/error threshold and other parameters</li> <li>Compute nonlinear deformation, generate grid</li> <li>Store grid in TransformDB</li> </ul> <p>Related code snippets: - afFlow2MultiDF.py#L21-L60 - afFlow2MultiDF.py#L128-L172</p>"},{"location":"alignment/image-alignment/#multi-center-registration-strategy","title":"Multi-center Registration Strategy","text":"<ul> <li>Select the middle section as the center, perform bidirectional registration</li> <li>Parallel processing to improve efficiency</li> <li>Merge transformations to ensure global consistency</li> </ul> <p>Related code snippets: - afFlow2MultiDF.py#L62-L89 - afFlow2MultiDF.py#L74-L126</p>"},{"location":"alignment/image-alignment/#transformation-management-and-composition","title":"Transformation Management and Composition","text":"<ul> <li>Transformations are stored as grids in TransformDB</li> <li>Supports grid composition and application to images/point sets</li> </ul> <p>Related code snippets: - afFlow2MultiDF.py#L128-L172</p>"},{"location":"alignment/image-alignment/#fine-tuning-and-manual-intervention","title":"Fine-tuning and Manual Intervention","text":"Component Purpose Implementation Method AutoScale Detect/correct scale differences Region of interest detection and scale calculation BestRotate Coarse/fine rotation search Multi-step coarse-to-fine rotation search FinalRotate Fine rotation adjustment Sliding window to find optimal rotation ManualRotate Manual rotation/scale/translation Provides interactive interface <p>Related code snippets: - AutoScale.py#L5-L26 - BestRotate.py#L5-L66 - FinalRotate.py#L5-L36 - ManualRotate.py#L6-L63</p>"},{"location":"alignment/image-alignment/#integration-with-3d-reconstruction","title":"Integration with 3D Reconstruction","text":"<ul> <li>Registration results are directly used for 3D reconstruction</li> <li>Supports section stacking, interpolation, and spatial analysis</li> </ul> <p>Related code snippets: - flow/init.py#L8-L14</p>"},{"location":"alignment/image-alignment/#summary","title":"Summary","text":"<p>The image alignment system in SpaceMap provides a solid foundation for 3D tissue reconstruction and spatial transcriptomics analysis through a multi-stage coarse-to-fine workflow, combining affine and LDDMM techniques, robust feature point matching and filtering, grid transformation management, and multi-center parallel strategies. </p>"},{"location":"analysis/spatial-transcriptomics-analysis/","title":"Spatial Transcriptomics Analysis","text":"<p>Related source files: - find/init.py - find/basic.py - find/cellShape.py - find/nearBound.py - find/nearBoundCellData.py - find/nearBoundThread.py - flow/afFlow2Multi.py - utils/compare/cell_metrix.py - utils/compare/filt.py - utils/he_img.py</p>"},{"location":"analysis/spatial-transcriptomics-analysis/#purpose-and-scope","title":"Purpose and Scope","text":"<p>This section documents the tools and methodologies for analyzing spatial transcriptomics data within the SpaceMap framework. The analysis capabilities enable researchers to quantify, compare, and validate gene expression patterns across aligned tissue sections. For information about the alignment process itself, see Image Alignment, and for the 3D reconstruction process, see 3D Reconstruction.</p>"},{"location":"analysis/spatial-transcriptomics-analysis/#overview-of-spatial-transcriptomics-analysis-components","title":"Overview of Spatial Transcriptomics Analysis Components","text":"<ul> <li>Expression analysis: grid-based quantification and normalization</li> <li>Layer distance calculation: quantifies similarity between adjacent sections</li> <li>Parallelized processing: efficient computation for large datasets</li> <li>Cell boundary and shape analysis: validates registration quality</li> <li>Cell type distribution analysis: compares cell type patterns across sections</li> </ul>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-expression-analysis","title":"Cell Expression Analysis","text":"<p>The cell expression analysis module processes spatial transcriptomics data to compute metrics that quantify gene expression patterns across tissue sections. The core of this functionality is implemented in the <code>cell_metrix.py</code> module.</p>"},{"location":"analysis/spatial-transcriptomics-analysis/#grid-based-expression-processing","title":"Grid-Based Expression Processing","text":"<p>Expression data is analyzed by dividing the tissue space into a grid and aggregating cell expression values within each grid cell:</p> <ul> <li>Divide the space into a grid with configurable size and overlap</li> <li>Sum expression values within each grid cell</li> <li>Normalize the expression values within each grid cell</li> </ul> <p>See: <code>cell_metrix.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#layer-distance-calculation","title":"Layer Distance Calculation","text":"<p>The <code>calculate_layer_distances</code> function computes expression similarity between adjacent tissue sections:</p> <ul> <li>For each layer pair, compare grid-processed expression matrices</li> <li>Calculate absolute differences between corresponding grid cells</li> <li>Normalize by the number of non-empty grid cells</li> <li>Mean distance across all layer pairs provides an alignment quality metric</li> </ul> <p>See: <code>cell_metrix.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#parallelized-processing","title":"Parallelized Processing","text":"<p>For large datasets, the framework provides <code>caculate_layer_distances_thread</code> for multi-threaded execution:</p> <ul> <li>Create parameter sets for each comparison (datasets, grid sizes, overlaps)</li> <li>Launch parallel workers using Python's multiprocessing</li> <li>Collect and aggregate results from all workers</li> </ul> <p>See: <code>cell_metrix.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-boundary-and-shape-analysis","title":"Cell Boundary and Shape Analysis","text":"<p>SpaceMap provides tools to analyze cell boundaries and shapes across aligned tissue sections, enabling validation of registration quality through cell morphology.</p>"},{"location":"analysis/spatial-transcriptomics-analysis/#nearest-cell-boundary-analysis","title":"Nearest Cell Boundary Analysis","text":"<p>The <code>NearBoundGenerate</code> class identifies corresponding cells between adjacent sections and computes boundary-based similarity metrics:</p> <ul> <li>Apply spatial transformations to cell boundaries using TransformDB</li> <li>Identify nearest neighboring cells between adjacent layers</li> <li>Compute boundary similarity metrics between matched cells</li> <li>Aggregate and export metrics for quality assessment</li> </ul> <p>See: <code>nearBound.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-shape-analysis","title":"Cell Shape Analysis","text":"<p>The <code>CellShapeGenerate</code> class extends boundary analysis with shape-specific metrics:</p> <ul> <li>Compare bounding box dimensions (min/max coordinates)</li> <li>Calculate aspect ratios before and after transformation</li> <li>Compute area preservation metrics</li> <li>Quantify overall shape similarity</li> </ul> <p>See: <code>cellShape.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-type-distribution-analysis","title":"Cell Type Distribution Analysis","text":"<p>SpaceMap provides tools to analyze the distribution of cell types across aligned tissue sections. This functionality helps validate alignment quality by ensuring similar cell type distributions in adjacent regions.</p>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-type-comparison-methods","title":"Cell Type Comparison Methods","text":"<ul> <li><code>cmp_layers_label</code>: Compares cell distributions by cell type labels</li> <li><code>cmp_filter_part</code>: Filters cells within a specific spatial region</li> <li><code>cmp_adjacent_layers</code>: Compares cell distributions in adjacent layers</li> <li><code>compare_workflow</code>: Orchestrates comparison between different datasets</li> </ul> <p>See: <code>filt.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#integration-with-registration-pipeline","title":"Integration with Registration Pipeline","text":"<p>The spatial transcriptomics analysis components are integrated with the broader registration pipeline through the <code>AutoFlowMultiCenter2</code> class, which coordinates the alignment of tissue sections:</p> <ul> <li>Transformations computed by the registration pipeline are stored in TransformDB</li> <li>Spatial transcriptomics analysis uses these transformations via the TransformDB API</li> <li>Quality metrics from the analysis can be used to refine registration parameters</li> <li>The multi-center approach in <code>AutoFlowMultiCenter2</code> starts from a center slice and works outward in both directions</li> </ul> <p>See: <code>afFlow2Multi.py</code></p>"},{"location":"analysis/spatial-transcriptomics-analysis/#practical-usage","title":"Practical Usage","text":""},{"location":"analysis/spatial-transcriptomics-analysis/#cell-expression-matrix-analysis","title":"Cell Expression Matrix Analysis","text":"<p>To calculate distances between expression patterns in adjacent layers:</p> <pre><code># Example usage for calculating layer distances\nfrom spacemap.utils.compare.cell_metrix import calculate_layer_distances\n# ... load your expression data ...\ndistances = calculate_layer_distances(expression_matrices)\n</code></pre> <p>For large datasets, use the parallelized version:</p> <pre><code>from spacemap.utils.compare.cell_metrix import caculate_layer_distances_thread\n# ... load your expression data ...\nresults = caculate_layer_distances_thread(expression_matrices)\n</code></pre>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-boundary-analysis","title":"Cell Boundary Analysis","text":"<p>To analyze cell boundaries across sections:</p> <pre><code>from spacemap.find.nearBound import NearBoundGenerate\n# ... prepare your cell boundary data ...\nnbg = NearBoundGenerate(...)\nnbg.run()\n</code></pre>"},{"location":"analysis/spatial-transcriptomics-analysis/#cell-type-distribution-comparison","title":"Cell Type Distribution Comparison","text":"<p>To compare cell type distributions across sections:</p> <pre><code>from spacemap.utils.compare.filt import cmp_layers_label\n# ... prepare your cell type data ...\nresult = cmp_layers_label(layer1, layer2)\n</code></pre> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"api/api/","title":"API Reference","text":"<p>This page provides an overview of SpaceMap's core modules and functions.</p>"},{"location":"api/api/#modules-overview","title":"Modules Overview","text":"<p>SpaceMap is organized into the following main modules:</p> Module Description <code>spacemap.flow</code> Core flow-based registration system <code>spacemap.registration</code> Registration algorithms <code>spacemap.affine</code> Affine transformation utilities <code>spacemap.matches</code> Feature detection and matching algorithms <code>spacemap.find</code> Point detection and analysis tools <code>spacemap.utils</code> Utility functions for data processing and visualization <code>spacemap.base</code> Core base classes and functions"},{"location":"api/api/#slice","title":"Slice","text":"<p>The <code>Slice</code> class is a fundamental component for handling tissue sections:</p> <pre><code>from spacemap import Slice\n\n# Key attributes\nSlice.align1Key  # Key for first alignment\nSlice.align2Key  # Key for second alignment\n\n# Main methods\nslice.get_transform_points()  # Get transformed points after registration\nslice.get_points()  # Get original points\n</code></pre>"},{"location":"api/api/#flow","title":"flow","text":"<p>The <code>flow</code> module implements the main registration workflow.</p> <pre><code># Main flow classes for registration workflow\nflow = spacemap.flow.FlowImport(base_dir)  # Initialize flow processing\nflow.init_xys(xys, layer_ids)  # Set xy coordinates and layer IDs\nslices = flow.slices  # Get slice objects\n\n# Registration manager\nmgr = spacemap.flow.AutoFlowMultiCenter3(slices)\nmgr.alignMethod = \"auto\"  # Set alignment method\nmgr.affine(method=\"DF\", show=True)  # Perform affine registration\nmgr.ldm_pair(Slice.align1Key, Slice.align2Key, show=True)  # Perform LDDMM registration\n\n# Export results\nexport = spacemap.flow.FlowExport(slices)\n# export.points_csv(\"output.csv\")  # Export transformed points to CSV\n\n# TransformDB for managing transformations\ndb = spacemap.flow.outputs.TransformDB()\ndb.add_transform(source_id, target_id, affine_transform, flow_transform)\ndb.save(filename)\ndb = spacemap.flow.outputs.TransformDB.load(filename)\n</code></pre>"},{"location":"api/api/#registration","title":"registration","text":"<p>The <code>registration</code> module provides functions for registering images and point sets.</p> <pre><code># Main registration functions\nspacemap.registration.affine_register(source_points, target_points, **kwargs)\nspacemap.registration.multimodal_register(source_points, target_points, source_image=None, target_image=None, **kwargs)\nspacemap.registration.global_optimize(transform_db, **kwargs)\nspacemap.registration.apply_transform_sequence(points, transform_db, source_id, target_id)\n</code></pre>"},{"location":"api/api/#affine","title":"affine","text":"<p>The <code>affine</code> module handles affine transformations.</p> <pre><code># Core affine functions\nspacemap.affine.transform_points(points, affine_matrix)\nspacemap.affine.transform_image(image, affine_matrix, output_shape=None)\nspacemap.affine.estimate_affine(source_points, target_points, method='ransac')\n</code></pre>"},{"location":"api/api/#matches","title":"matches","text":"<p>The <code>matches</code> module provides feature matching algorithms.</p> <pre><code># Feature matching\nkeypoints1, keypoints2, matches = spacemap.matches.find_matches(image1, image2, method='sift')\nspacemap.matches.filter_matches(keypoints1, keypoints2, matches, **kwargs)\n</code></pre>"},{"location":"api/api/#find","title":"find","text":"<p>The <code>find</code> module contains tools for detecting and analyzing points.</p> <pre><code># Point detection and error analysis\nkeypoints = spacemap.find.detect_keypoints(image, method='sift')\nerrors = spacemap.find.err.calculate_registration_error(source_points, target_points, transformed_points)\n</code></pre>"},{"location":"api/api/#utils","title":"utils","text":"<p>The <code>utils</code> module provides various utility functions.</p>"},{"location":"api/api/#utilsshow","title":"utils.show","text":"<p>Visualization utilities.</p> <pre><code># Visualization functions\nspacemap.utils.show.plot_image(image, **kwargs)\nspacemap.utils.show.plot_points(points, **kwargs)\nspacemap.utils.show.plot_matches(image1, image2, keypoints1, keypoints2, matches, **kwargs)\nspacemap.utils.show.plot_transformation_field(transform, **kwargs)\nspacemap.utils.show.interactive_3d_plot(points_3d, **kwargs)\n</code></pre>"},{"location":"api/api/#utilsimg","title":"utils.img","text":"<p>Image processing utilities.</p> <pre><code># Image utilities\nimage = spacemap.utils.img.load_image(filename)\nprocessed_image = spacemap.utils.img.preprocess_image(image, **kwargs)\nspacemap.utils.img.save_image(image, filename)\n</code></pre>"},{"location":"api/api/#utilscompute","title":"utils.compute","text":"<p>Computation utilities.</p> <pre><code># Computational utilities\nnormalized_points = spacemap.utils.compute.normalize_points(points)\ndistances = spacemap.utils.compute.pairwise_distances(points1, points2)\n</code></pre>"},{"location":"api/api/#utilscompare","title":"utils.compare","text":"<p>Comparison utilities.</p> <pre><code># Comparison utilities\nspacemap.utils.compare.interactive_comparison(source_points, target_points, aligned_points, **kwargs)\n</code></pre>"},{"location":"api/api/#utilsgrid_points","title":"utils.grid_points","text":"<p>Utilities for grid-based operations on point sets.</p> <pre><code># Grid operations\ngrid = spacemap.utils.grid_points.points_to_grid(points, grid_size)\ninterpolated_points = spacemap.utils.grid_points.interpolate_points(points, values, query_points)\n</code></pre>"},{"location":"api/api/#utilsimaris","title":"utils.imaris","text":"<p>Utilities for working with Imaris files.</p> <pre><code># Imaris utilities\nspacemap.utils.imaris.export_points(points, filename, **kwargs)\n</code></pre>"},{"location":"api/api/#base","title":"base","text":"<p>The <code>base</code> module contains core base classes and functionality.</p> <pre><code># Core settings\nspacemap.base.root.DEVICE = 0  # Set to 'cpu' or GPU device number\nspacemap.base.root.L  # Global logger\n\n# Flow base\nspacemap.base.flowBase.FlowBase  # Base class for flow transforms\n</code></pre>"},{"location":"api/api/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's an overview of a complete registration workflow:</p> <pre><code>import spacemap\nfrom spacemap import Slice\nimport pandas as pd\n\n# 1. Load data\ndf = pd.read_csv(\"cells.csv.gz\")\ngroups = df.groupby(\"layer\")\nxys = []  # xy coordinates\nids = []  # layer IDs\n\nfor layer, dff in groups:\n    xy = dff[[\"x\", \"y\"]].values\n    ids.append(layer)\n    xys.append(xy)\n\n# 2. Initialize flow processing\nbase = \"data/flow\"\nflow = spacemap.flow.FlowImport(base)\nflow.init_xys(xys, ids)\nslices = flow.slices\n\n# 3. Perform registrations\nmgr = spacemap.flow.AutoFlowMultiCenter3(slices)\nmgr.alignMethod = \"auto\"\nmgr.affine(\"DF\", show=True)\nmgr.ldm_pair(Slice.align1Key, Slice.align2Key, show=True)\n\n# 4. Export results\nexport = spacemap.flow.FlowExport(slices)\n</code></pre>"},{"location":"api/api/#examples","title":"Examples","text":"<p>For complete usage examples, refer to:</p> <ul> <li>Quick Start Guide</li> <li>Usage Guide</li> <li>Examples</li> </ul>"},{"location":"api/api/#detailed-api-documentation","title":"Detailed API Documentation","text":"<p>For detailed function signatures, parameters, and return types, refer to the auto-generated API documentation. </p>"},{"location":"architecture/architecture-and-core-components/","title":"Architecture and Core Components","text":"<p>Related source files: - init.py - base/base2.py - base/root.py - flow/afFlowMultiCenter3.py - flow/outputs.py - docs/assets/images/logo.png - docs/assets/images/qr.png - docs/examples/examples.md - docs/index.md - docs/examples/usage.md - mkdocs.yml</p>"},{"location":"architecture/architecture-and-core-components/#introduction","title":"Introduction","text":"<p>This page provides a high-level overview of the SpaceMap architecture and its main components. SpaceMap is designed as a modular framework focused on reconstructing 3D tissue maps from consecutive tissue sections, particularly suitable for spatial transcriptomics and proteomics data.</p> <p>For detailed component descriptions, please refer to the sub-pages: - Registration System (LDDMM) - Grid Transformations - Data Management (Flow, Slice) - Feature Matching</p>"},{"location":"architecture/architecture-and-core-components/#system-architecture-overview","title":"System Architecture Overview","text":"<p>SpaceMap employs a layered architecture that decouples data management, registration, and visualization while ensuring efficient data flow.</p>"},{"location":"architecture/architecture-and-core-components/#core-components","title":"Core Components","text":"<p>SpaceMap consists of multiple interconnected components that work together to process and align tissue section data. The main classes and their relationships are as follows:</p> <ul> <li>Registration System: Core registration system, divided into:</li> <li>Affine Registration: Global coarse alignment, handling translation, rotation, scaling, and shearing.</li> <li>LDDMM Registration: Fine-grained nonlinear registration, preserving local structures.</li> <li>Grid Transformations: Fundamental mechanism for representing and applying nonlinear deformations, supporting forward/inverse transformations, bilinear interpolation, point cloud/image transformations, and transformation composition.</li> <li>Data Management (Flow, Slice):</li> <li>FlowImport: Loads raw data (cell coordinates, images), initializes the workflow.</li> <li>Slice: Individual section object, containing raw data and all transformations.</li> <li>TransformDB: Manages all transformations between sections.</li> <li>FlowExport: Exports aligned sections and 3D models.</li> <li>Feature Matching:</li> <li>SIFT: Traditional feature point matching.</li> <li>LOFTR: Transformer-based deep learning feature matching.</li> </ul>"},{"location":"architecture/architecture-and-core-components/#data-processing-flow","title":"Data Processing Flow","text":"<p>The standard workflow is as follows: 1. FlowImport loads data 2. Slice represents each section 3. TransformDB manages all transformations 4. FlowExport exports results</p>"},{"location":"architecture/architecture-and-core-components/#multi-resolution-strategy","title":"Multi-resolution Strategy","text":"<p>SpaceMap employs a multi-resolution registration strategy (e.g., AutoFlowMultiCenter3 class) to process at different resolutions, balancing efficiency and accuracy.</p>"},{"location":"architecture/architecture-and-core-components/#transformdb-unified-transformation-management","title":"TransformDB: Unified Transformation Management","text":"<p>TransformDB provides a unified interface supporting: - Affine and grid transformations - Forward/inverse transformations - Automatic scaling to match input dimensions - Batch processing of multiple images/point sets</p>"},{"location":"architecture/architecture-and-core-components/#configuration-and-global-settings","title":"Configuration and Global Settings","text":"<p>Global configuration is defined in base/root.py, including:</p> Setting Purpose Default Value XYRANGE Maximum coordinate range 4000 XYD Coordinate transformation scale factor 10 BASE Data storage directory \"data/flow\" DEVICE Computing device (CPU/GPU) \"cpu\" or 0 LAYER_START Starting layer index 0 LAYER_END Ending layer index 0 IMGCONF Image configuration {\"raw\": 1} <p>The <code>auto_init_xyd</code> and <code>init_xy</code> in base/base2.py can automatically initialize based on input data.</p>"},{"location":"architecture/architecture-and-core-components/#component-interaction","title":"Component Interaction","text":"<p>The diagram (original class/flow diagram, suggested to add images in docs directory) shows the interaction relationships between core components in a typical registration workflow.</p>"},{"location":"architecture/architecture-and-core-components/#system-dependencies","title":"System Dependencies","text":"<ul> <li>Requires Python and related scientific computing libraries (e.g., numpy, torch, opencv, kornia)</li> <li>Detailed dependencies can be found in the project's requirements.txt</li> </ul> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"data/data-management/","title":"Data Management (Flow, Slice)","text":"<p>Related source files: - base/base2.py - base/root.py - flow/afFlowMultiCenter3.py - flow/flowExport.py - flow/flowImport.py - flow/outputs.py - matches/siftFind.py</p>"},{"location":"data/data-management/#introduction","title":"Introduction","text":"<p>The data management components of SpaceMap provide infrastructure for importing, organizing, processing, and exporting tissue section data. Flow and Slice are the core data management components for the 3D tissue reconstruction workflow.</p> <p>For more about the transformation system, see: Registration System (LDDMM) and Grid Transformations.</p>"},{"location":"data/data-management/#overview","title":"Overview","text":"<ul> <li>Flow Class: Responsible for data import (FlowImport) and export (FlowExport)</li> <li>Slice Class: Manages individual sections and their related data</li> </ul> <p>Together, they build a data lifecycle management framework for the entire tissue reconstruction process.</p>"},{"location":"data/data-management/#class-structure-and-relationships","title":"Class Structure and Relationships","text":"<ul> <li>FlowImport: Initializes data processing workflow, creates Slice objects and configures global coordinate system</li> <li>FlowExport: Exports processing results, including transformation parameters and processed images</li> <li>Slice: Data structure for individual sections, managing points, images, transformations, etc.</li> <li>TransformDB: Unified management of affine and grid transformations, supporting batch application</li> </ul>"},{"location":"data/data-management/#data-flow","title":"Data Flow","text":"<p>Data flows through the SpaceMap system as follows: 1. Import data, initialize sections 2. Process (registration, feature extraction, etc.) 3. Export results 4. Apply transformations to new data</p>"},{"location":"data/data-management/#key-components-and-methods","title":"Key Components and Methods","text":""},{"location":"data/data-management/#flowimport","title":"FlowImport","text":"<ul> <li><code>init_xys(xys, ids=None)</code>: Initialize sections based on coordinate arrays, automatically calculate coordinate range and resolution, save global parameters</li> <li><code>auto_init()</code>: Read parameters from conf.json, reconstruct section objects</li> <li><code>init_from_codex(csvPath)</code>: Support CODEX format data import</li> <li>Global parameters:</li> <li><code>XYRANGE</code>: Data coordinate range</li> <li><code>XYD</code>: Coordinate transformation resolution</li> </ul>"},{"location":"data/data-management/#flowexport","title":"FlowExport","text":"<ul> <li><code>export_affine_grid(affineShape=None, gridKey1=\"final_ldm\", gridKey2=\"img\", save=None)</code>: Export affine and grid transformations, support saving as NPZ</li> <li><code>export_imgs(key, mchannel=True, he=False, scale=False)</code>: Export processed images for all sections, support multi-channel and H&amp;E</li> <li><code>import_imgs(key, imgs)</code>: Import external images and distribute to sections</li> </ul>"},{"location":"data/data-management/#slice","title":"Slice","text":"<ul> <li>Manages single section's points, images, transformations, etc.</li> <li>Supports point coordinate image generation, affine/grid transformation application, disk I/O</li> <li>Bridges data management and registration systems</li> </ul>"},{"location":"data/data-management/#transformdb","title":"TransformDB","text":"<ul> <li>Unified interface for managing affine and grid transformations</li> <li><code>apply_img(img, index)</code>: Apply transformation to image</li> <li><code>apply_point(p, index, maxShape=None)</code>: Apply transformation to point coordinates</li> <li><code>apply_imgs(imgs)</code>/<code>apply_points(ps)</code>: Batch processing</li> </ul>"},{"location":"data/data-management/#data-storage-structure","title":"Data Storage Structure","text":"<pre><code>basePath/\n\u251c\u2500\u2500 conf.json        # Global configuration parameters\n\u251c\u2500\u2500 imgs/            # Processed images\n\u251c\u2500\u2500 outputs/         # Transformation outputs\n\u251c\u2500\u2500 raw/             # Raw input data\n\u2514\u2500\u2500 logs/            # Logs\n</code></pre> <ul> <li>conf.json stores core parameters like XYRANGE, XYD</li> </ul>"},{"location":"data/data-management/#multi-resolution-processing","title":"Multi-resolution Processing","text":"<p>Supports multi-resolution processing through dynamic XYD parameter adjustment, balancing large data efficiency and detail preservation.</p>"},{"location":"data/data-management/#file-formats","title":"File Formats","text":"Format Description Usage NPZ Compressed numpy arrays Store transformation matrices and grids JSON Configuration files Store global parameters and section info Images PNG, TIFF, etc. Input/output images CSV Tabular data External data import (e.g., CODEX)"},{"location":"data/data-management/#integration-with-other-components","title":"Integration with Other Components","text":"<ul> <li>Registration System: Uses sections as data containers, writes transformation results</li> <li>Feature Matching: Operates on images generated by Slice</li> <li>Grid Transformations: Generated transformations stored and applied through data management system</li> </ul> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"examples/examples/","title":"Examples","text":"<p>This page provides practical examples of using Space-map for different applications.</p>"},{"location":"examples/examples/#human-colon-polyp-study","title":"Human Colon Polyp Study","text":"<p>Space-map was used to reconstruct 3D models of human colon tissues using spatial transcriptomics and proteomics data. The dataset included ~2.9M cells from Xenium and ~2.4M cells from CODEX.</p> <pre><code>import spacemap as sm\nimport pandas as pd\n\n# Load and process Xenium data\nxenium_data = pd.read_csv('xenium_data.csv')\nxenium_slices = sm.process_xenium(xenium_data)\n\n# Load and process CODEX data\ncodex_data = pd.read_csv('codex_data.csv')\ncodex_slices = sm.process_codex(codex_data)\n\n# Perform registration\nmgr = sm.flow.AutoFlowMultiCenter3(xenium_slices + codex_slices)\nmgr.alignMethod = \"auto\"\nmgr.affine(\"DF\", show=True)\nmgr.ldm_pair(sm.Slice.align1Key, sm.Slice.align2Key, show=True)\n\n# Export results\nexport = sm.flow.FlowExport(mgr.slices)\nexport.export_3d_model('colon_polyp_3d_model')\n</code></pre>"},{"location":"examples/examples/#example-1-complete-registration-workflow-with-csv-data","title":"Example 1: Complete Registration Workflow with CSV Data","text":"<p>This example demonstrates a complete workflow for registering multiple tissue sections using cell coordinates from a CSV file.</p> <pre><code>import spacemap\nfrom spacemap import Slice\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Load data from CSV file\n# The CSV file contains cell coordinates with layer information\ndf = pd.read_csv(\"path/to/cells.csv.gz\")\ngroups = df.groupby(\"layer\")\n\n# Organize data by layers\nxys = []  # List to store coordinates for each layer\nids = []  # List to store layer IDs\n\nfor layer, dff in groups:\n    xy = dff[[\"x\", \"y\"]].values\n    ids.append(layer)\n    xys.append(xy)\n\n# Step 2: Set up the project\n# Define base folder for the project\nbase = \"data/flow\"\n\n# Initialize the flow processing system\nflow = spacemap.flow.FlowImport(base)\n\n# Initialize with xy coordinates and layer IDs\nflow.init_xys(xys, ids)\n\n# Get slice objects for further processing\nslices = flow.slices\n\n# Step 3: Perform affine registration for coarse alignment\n# Create registration manager\nmgr = spacemap.flow.AutoFlowMultiCenter3(slices)\n\n# Set alignment method to automatic\nmgr.alignMethod = \"auto\"\n\n# Perform affine registration\n# \"DF\" specifies the method to use\n# show=True displays visualization during the process\nmgr.affine(\"DF\", show=True)\n\n# Step 4: Perform LDDMM (Large Deformation Diffeomorphic Metric Mapping)\n# This provides high-precision alignment while preserving local structures\nmgr.ldm_pair(Slice.align1Key, Slice.align2Key, show=True)\n\n# Step 5: Export the results\nexport = spacemap.flow.FlowExport(slices)\n\n# Export can be used to save transformed coordinates to CSV\n# export.points_csv(\"aligned_cells.csv\")\n\n# Step 6: Visualize the aligned sections\n# Create a 3D visualization\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\n\n# Define colors for different layers\ncolors = plt.cm.viridis(np.linspace(0, 1, len(slices)))\n\n# Plot each layer\nfor i, s in enumerate(slices):\n    # Get transformed points\n    points = s.get_transform_points()\n\n    # Add z-coordinate based on layer index\n    z_coord = np.ones(len(points)) * i * 10  # 10 units spacing between layers\n    points_3d = np.column_stack([points, z_coord])\n\n    # Plot points\n    ax.scatter(\n        points_3d[:, 0], \n        points_3d[:, 1], \n        points_3d[:, 2],\n        color=colors[i],\n        s=2,\n        alpha=0.7,\n        label=f\"Layer {s.id}\"\n    )\n\n# Set labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z (Layer)')\nax.legend()\nplt.title('3D Reconstruction of Aligned Tissue Sections')\nplt.show()\n\n# Optional: Analyze alignment quality\n# Calculate average distance between matched points\ntotal_error = 0\ntotal_points = 0\n\nfor i in range(len(slices)-1):\n    # Get points from adjacent slices\n    points1 = slices[i].get_transform_points()\n    points2 = slices[i+1].get_transform_points()\n\n    # Calculate distances (simplified example)\n    # In a real scenario, you would use matched points rather than all points\n    min_len = min(len(points1), len(points2))\n    points1 = points1[:min_len]\n    points2 = points2[:min_len]\n\n    # Calculate mean squared error\n    mse = np.mean(np.sum((points1 - points2)**2, axis=1))\n    total_error += mse * min_len\n    total_points += min_len\n\n# Print average alignment error\navg_error = np.sqrt(total_error / total_points) if total_points &gt; 0 else 0\nprint(f\"Average alignment error: {avg_error:.4f} units\")\n</code></pre>"},{"location":"examples/examples/#example-2-basic-registration-of-two-sections","title":"Example 2: Basic Registration of Two Sections","text":"<p>This example demonstrates how to register two adjacent tissue sections containing cell coordinates.</p> <pre><code>import spacemap as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Load data\n# Simulate two sections with slight displacement and deformation\nnp.random.seed(42)\nn_points = 1000\n\n# Create source points\nsource_points = np.random.rand(n_points, 2) * 100\n\n# Create target points (slightly shifted and deformed)\nangle = np.pi / 15  # Small rotation\nscale = 1.1  # Small scaling\ntranslation = np.array([5, 3])  # Translation vector\n\n# Apply transformation to create target points\nrotation_matrix = np.array([\n    [np.cos(angle), -np.sin(angle)],\n    [np.sin(angle), np.cos(angle)]\n])\ntarget_points = (scale * (rotation_matrix @ source_points.T).T + \n                 translation + \n                 np.random.normal(0, 2, (n_points, 2)))  # Add noise\n\n# 2. Perform registration\n# Step 1: Affine registration\naffine_transform = sm.registration.affine_register(\n    source_points=source_points,\n    target_points=target_points\n)\n\n# Apply affine transformation\nsource_points_affine = sm.affine.transform_points(source_points, affine_transform)\n\n# Step 2: Flow-based fine registration\nflow_transform = sm.flow.register(\n    source_points=source_points_affine,\n    target_points=target_points,\n    iterations=100\n)\n\n# Apply flow transformation\nsource_points_final = sm.flow.transform_points(source_points_affine, flow_transform)\n\n# 3. Visualize results\nplt.figure(figsize=(15, 5))\n\n# Original points\nplt.subplot(1, 3, 1)\nplt.scatter(source_points[:, 0], source_points[:, 1], s=5, alpha=0.5, label=\"Source\")\nplt.scatter(target_points[:, 0], target_points[:, 1], s=5, alpha=0.5, label=\"Target\")\nplt.title(\"Original Points\")\nplt.legend()\n\n# After affine registration\nplt.subplot(1, 3, 2)\nplt.scatter(source_points_affine[:, 0], source_points_affine[:, 1], s=5, alpha=0.5, label=\"Source (Affine)\")\nplt.scatter(target_points[:, 0], target_points[:, 1], s=5, alpha=0.5, label=\"Target\")\nplt.title(\"After Affine Registration\")\nplt.legend()\n\n# After flow-based registration\nplt.subplot(1, 3, 3)\nplt.scatter(source_points_final[:, 0], source_points_final[:, 1], s=5, alpha=0.5, label=\"Source (Final)\")\nplt.scatter(target_points[:, 0], target_points[:, 1], s=5, alpha=0.5, label=\"Target\")\nplt.title(\"After Flow Registration\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 4. Evaluate registration quality\ninitial_error = np.mean(np.sqrt(np.sum((source_points - target_points)**2, axis=1)))\naffine_error = np.mean(np.sqrt(np.sum((source_points_affine - target_points)**2, axis=1)))\nfinal_error = np.mean(np.sqrt(np.sum((source_points_final - target_points)**2, axis=1)))\n\nprint(f\"Initial average distance error: {initial_error:.2f}\")\nprint(f\"After affine registration: {affine_error:.2f}\")\nprint(f\"After flow-based registration: {final_error:.2f}\")\n</code></pre>"},{"location":"examples/examples/#example-3-multi-section-registration-with-feature-matching","title":"Example 3: Multi-section Registration with Feature Matching","text":"<p>This example shows how to register multiple sections using both cell coordinates and image features.</p> <pre><code>import spacemap as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io\nfrom skimage.transform import resize\n\n# 1. Load data (here we simulate 3 sections)\nn_sections = 3\nn_points_per_section = 800\nsection_points = []\nsection_images = []\n\nnp.random.seed(42)\n\n# Simulate section data\nfor i in range(n_sections):\n    # Generate random points with some structure (e.g., cluster pattern)\n    centers = np.random.rand(5, 2) * 100\n    points = np.vstack([\n        np.random.normal(center, scale=10, size=(n_points_per_section // 5, 2))\n        for center in centers\n    ])\n\n    # Add section-specific transformation\n    angle = np.random.uniform(-np.pi/10, np.pi/10)\n    scale = np.random.uniform(0.9, 1.1)\n    translation = np.random.uniform(-10, 10, size=2)\n\n    rotation_matrix = np.array([\n        [np.cos(angle), -np.sin(angle)],\n        [np.sin(angle), np.cos(angle)]\n    ])\n\n    points = scale * (rotation_matrix @ points.T).T + translation\n    section_points.append(points)\n\n    # Create a simple image with point density\n    img_size = 500\n    img = np.zeros((img_size, img_size))\n\n    # Scale points to image coordinates\n    img_points = np.clip((points - points.min(axis=0)) / (points.max(axis=0) - points.min(axis=0)) * (img_size-1), 0, img_size-1).astype(int)\n\n    # Add points to image with Gaussian blur to simulate staining\n    for x, y in img_points:\n        img[y, x] = 1\n\n    # Add some simulated tissue structures\n    for _ in range(20):\n        cx, cy = np.random.randint(0, img_size, 2)\n        radius = np.random.randint(10, 50)\n        for y in range(max(0, cy - radius), min(img_size, cy + radius)):\n            for x in range(max(0, cx - radius), min(img_size, cx + radius)):\n                if ((x - cx)**2 + (y - cy)**2) &lt; radius**2:\n                    img[y, x] = 0.5\n\n    # Resize and normalize\n    img = resize(img, (200, 200))\n    section_images.append(img)\n\n# 2. Register sections\n# We'll register each section to the next one, building a 3D stack\ntransform_db = sm.flow.outputs.TransformDB()\n\nfor i in range(n_sections - 1):\n    source_points = section_points[i]\n    target_points = section_points[i+1]\n    source_img = section_images[i]\n    target_img = section_images[i+1]\n\n    # Find image feature matches\n    keypoints_source, keypoints_target, matches = sm.matches.find_matches(\n        source_img, target_img, method=\"sift\"\n    )\n\n    # Affine registration\n    affine_transform = sm.registration.affine_register(\n        source_points=source_points,\n        target_points=target_points\n    )\n\n    # Apply affine transformation\n    source_points_affine = sm.affine.transform_points(source_points, affine_transform)\n\n    # Flow-based registration using both points and keypoints\n    flow_transform = sm.flow.register(\n        source_points=source_points_affine,\n        target_points=target_points,\n        source_keypoints=keypoints_source,\n        target_keypoints=keypoints_target,\n        matches=matches,\n        iterations=150\n    )\n\n    # Store transforms\n    transform_db.add_transform(\n        source_id=i,\n        target_id=i+1,\n        affine_transform=affine_transform,\n        flow_transform=flow_transform\n    )\n\n    # Transform source points for visualization\n    section_points[i] = sm.flow.transform_points(source_points_affine, flow_transform)\n\n# 3. Create 3D model\nmodel_3d = []\nfor i, points in enumerate(section_points):\n    # Add z-coordinate (section index)\n    points_3d = np.column_stack([points, np.ones(len(points)) * i * 10])  # 10 units spacing between sections\n    model_3d.append(points_3d)\n\nmodel_3d = np.vstack(model_3d)\n\n# 4. Visualize 3D model\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Color points by section\ncolors = plt.cm.viridis(np.linspace(0, 1, n_sections))\nfor i, points in enumerate(section_points):\n    points_3d = np.column_stack([points, np.ones(len(points)) * i * 10])\n    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], \n               color=colors[i], s=1, alpha=0.7, label=f\"Section {i+1}\")\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z (Section)')\nax.legend()\nplt.title('3D Tissue Model')\nplt.show()\n</code></pre>"},{"location":"examples/examples/#example-3-working-with-real-data","title":"Example 3: Working with Real Data","text":"<p>This example demonstrates working with real data from files.</p> <pre><code>import spacemap as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# Assuming you have these data files:\n# - section1.csv, section2.csv, etc. with columns 'x', 'y', 'cell_type'\n# - section1.tif, section2.tif, etc. as histology images\n\n# 1. Load data\ndata_dir = \"path/to/data\"\nn_sections = 3  # Number of sections to process\nsections_data = []\n\nfor i in range(1, n_sections + 1):\n    # Load cell coordinates and metadata\n    cell_file = os.path.join(data_dir, f\"section{i}.csv\")\n    cells_df = pd.read_csv(cell_file)\n\n    # Extract coordinates\n    points = cells_df[['x', 'y']].values\n\n    # Load histology image if available\n    image_file = os.path.join(data_dir, f\"section{i}.tif\")\n    img = None\n    if os.path.exists(image_file):\n        img = sm.utils.img.load_image(image_file)\n\n    sections_data.append({\n        \"points\": points,\n        \"image\": img,\n        \"metadata\": cells_df\n    })\n\n# 2. Register adjacent sections\ntransform_db = sm.flow.outputs.TransformDB()\naligned_sections = [sections_data[0]]  # First section is reference\n\nfor i in range(n_sections - 1):\n    source = sections_data[i]\n    target = sections_data[i+1]\n\n    print(f\"Registering section {i+1} to {i+2}...\")\n\n    # Perform affine registration\n    affine_transform = sm.registration.affine_register(\n        source_points=source[\"points\"],\n        target_points=target[\"points\"]\n    )\n\n    # Apply affine transformation\n    source_points_affine = sm.affine.transform_points(source[\"points\"], affine_transform)\n\n    # Perform flow-based registration if images are available\n    if source[\"image\"] is not None and target[\"image\"] is not None:\n        # Find image feature matches\n        keypoints_source, keypoints_target, matches = sm.matches.find_matches(\n            source[\"image\"], target[\"image\"], method=\"loftr\"\n        )\n\n        # Perform flow registration using both points and keypoints\n        flow_transform = sm.flow.register(\n            source_points=source_points_affine,\n            target_points=target[\"points\"],\n            source_keypoints=keypoints_source,\n            target_keypoints=keypoints_target,\n            matches=matches,\n            iterations=200\n        )\n    else:\n        # Use only points for registration\n        flow_transform = sm.flow.register(\n            source_points=source_points_affine,\n            target_points=target[\"points\"],\n            iterations=200\n        )\n\n    # Store transforms\n    transform_db.add_transform(\n        source_id=i,\n        target_id=i+1,\n        affine_transform=affine_transform,\n        flow_transform=flow_transform\n    )\n\n    # Apply transform to the target section\n    aligned_section = dict(target)\n    aligned_section[\"points\"] = sm.flow.transform_points(\n        target[\"points\"], \n        flow_transform\n    )\n\n    # Store aligned section\n    aligned_sections.append(aligned_section)\n\n# 3. Create 3D model with cell type information\nmodel_3d = []\ncell_types = []\n\nfor i, section in enumerate(aligned_sections):\n    # Add z-coordinate (section index)\n    points = section[\"points\"]\n    points_3d = np.column_stack([points, np.ones(len(points)) * i * 10])\n    model_3d.append(points_3d)\n\n    # Extract cell types if available\n    if \"metadata\" in section and \"cell_type\" in section[\"metadata\"].columns:\n        cell_types.extend(section[\"metadata\"][\"cell_type\"].tolist())\n\nmodel_3d = np.vstack(model_3d)\n\n# 4. Visualize 3D model colored by cell type\nif cell_types:\n    unique_types = list(set(cell_types))\n    type_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n    type_to_color = {t: type_colors[i] for i, t in enumerate(unique_types)}\n\n    point_colors = [type_to_color[t] for t in cell_types]\n\n    fig = plt.figure(figsize=(12, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Plot each cell type separately for legend\n    for i, cell_type in enumerate(unique_types):\n        mask = np.array(cell_types) == cell_type\n        if np.any(mask):\n            ax.scatter(\n                model_3d[mask, 0], \n                model_3d[mask, 1], \n                model_3d[mask, 2],\n                color=type_colors[i],\n                s=3,\n                alpha=0.7,\n                label=cell_type\n            )\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z (Section)')\n    ax.legend()\n    plt.title('3D Tissue Model with Cell Types')\n    plt.show()\n\n# 5. Save the model\noutput_dir = os.path.join(data_dir, \"results\")\nos.makedirs(output_dir, exist_ok=True)\n\n# Save transformation database\ntransform_db.save(os.path.join(output_dir, \"transforms.pkl\"))\n\n# Save 3D model\nnp.save(os.path.join(output_dir, \"model_3d.npy\"), model_3d)\n\n# Save as CSV with metadata\nif cell_types:\n    model_df = pd.DataFrame({\n        'x': model_3d[:, 0],\n        'y': model_3d[:, 1],\n        'z': model_3d[:, 2],\n        'cell_type': cell_types\n    })\n    model_df.to_csv(os.path.join(output_dir, \"model_3d.csv\"), index=False)\n\nprint(f\"Results saved to {output_dir}\")\n</code></pre> <p>For more detailed information about the registration process, see the Registration System documentation.</p> <p>For information about data management and preprocessing, refer to the Data Management guide. </p>"},{"location":"examples/usage/","title":"Architecture and Core Components","text":"<p>This section provides a high-level overview of the SpaceMap architecture and its primary components. SpaceMap is designed as a modular framework for reconstructing 3D tissue maps from serial tissue sections, with a particular focus on spatial transcriptomics and proteomics data.</p> <p>For detailed information about specific components, see the following sub-sections: - Registration System (LDDMM) - Grid Transformations - Data Management (Flow, Slice) - Feature Matching</p>"},{"location":"examples/usage/#system-architecture-overview","title":"System Architecture Overview","text":"<p>SpaceMap employs a layered architecture that separates data management, registration, and visualization concerns while maintaining efficient data flow between components.</p>"},{"location":"examples/usage/#core-components","title":"Core Components","text":"<p>SpaceMap consists of several interconnected components that work together to process and align tissue section data: - FlowImport: Responsible for loading raw data (cell coordinates, images) and initializing the processing pipeline. - Slice: Represents a single tissue section, containing both the raw data and any transformations applied to it. - TransformDB: Stores and manages transformations between different sections. - FlowExport: Handles exporting of aligned sections and 3D models.</p>"},{"location":"examples/usage/#registration-system","title":"Registration System","text":"<p>The registration system is the core of SpaceMap, responsible for aligning tissue sections. It consists of two primary components: 1. Affine Registration: Provides coarse alignment by computing global transformations that account for translation, rotation, scaling, and shearing. 2. LDDMM Registration: Performs fine-grained, non-linear registration that preserves local structures while accurately aligning sections.</p> <p>The two-stage approach combines computational efficiency with high accuracy, allowing SpaceMap to handle large datasets while maintaining precision.</p> <p>For more detailed information about the registration process, see the Registration System documentation.</p>"},{"location":"examples/usage/#grid-transformations","title":"Grid Transformations","text":"<p>Grid transformations are a fundamental mechanism in SpaceMap for representing and applying non-linear deformations. These transformations are represented as regular grids where each point maps to another location, enabling complex warping operations.</p> <p>Key features: - Supports both forward and inverse transformations - Provides bilinear interpolation for points between grid vertices - Can be applied to both images and point clouds - Facilitates combination of multiple transformations</p>"},{"location":"examples/usage/#data-management-flow-slice","title":"Data Management (Flow, Slice)","text":"<p>The data management components in SpaceMap handle the import, manipulation, and export of tissue section data. The primary classes are: - FlowImport: Responsible for loading raw data (cell coordinates, images) and initializing the processing pipeline. - Slice: Represents a single tissue section, containing both the raw data and any transformations applied to it. - TransformDB: Stores and manages transformations between different sections. - FlowExport: Handles exporting of aligned sections and 3D models.</p> <p>For information about data management and preprocessing, refer to the Data Management guide.</p>"},{"location":"examples/usage/#feature-matching","title":"Feature Matching","text":"<p>Feature matching components are used to find correspondences between different tissue sections, which are then used as constraints in the registration process: - SIFT Matching: Traditional computer vision approach for matching distinctive points between images. - LOFTR (Learning Optical Flow from Transformers): Deep learning-based method for feature matching, particularly effective for histological images.</p>"},{"location":"examples/usage/#multi-resolution-strategy","title":"Multi-Resolution Strategy","text":"<p>SpaceMap employs a multi-resolution approach to registration to balance efficiency and accuracy. The <code>AutoFlowMultiCenter3</code> class in particular implements this strategy, enabling processing at multiple resolution levels for optimal results.</p>"},{"location":"examples/usage/#transformdb-unified-transformation-management","title":"TransformDB: Unified Transformation Management","text":"<p>The <code>TransformDB</code> class plays a central role in SpaceMap by providing a unified interface for storing and applying transformations: - Supports both affine and grid-based transformations - Forward and inverse transformations - Automatic scaling to match input dimensions - Batch processing for multiple images or point sets</p>"},{"location":"examples/usage/#configuration-and-global-settings","title":"Configuration and Global Settings","text":"<p>SpaceMap uses global configuration settings to control various aspects of the system. These are defined in <code>base/root.py</code> and include:</p> Setting Purpose Default Value XYRANGE Maximum coordinate range 4000 XYD Scale factor for coordinate transforms 10 BASE Base directory for data storage \"data/flow\" DEVICE Computing device (CPU or GPU index) \"cpu\" or 0 LAYER_START Starting layer index 0 LAYER_END Ending layer index 0 IMGCONF Image configuration settings {\"raw\": 1} <p>The <code>auto_init_xyd</code> and <code>init_xy</code> functions in <code>base/base2.py</code> provide convenient ways to initialize these settings based on input data.</p>"},{"location":"examples/usage/#key-workflows","title":"Key Workflows","text":"<p>This section outlines the primary processing workflows in the SpaceMap framework for 3D tissue reconstruction. It provides a high-level overview of the main processing pipelines, their components, and how they interact. The workflows described here cover the entire process from raw data import to final 3D model generation and spatial analysis.</p>"},{"location":"examples/usage/#main-workflows-overview","title":"Main Workflows Overview","text":"<p>SpaceMap implements a multi-stage approach to 3D tissue reconstruction, combining multi-scale feature matching with large-deformation diffeomorphic metric mapping (LDDMM). The core workflows are designed to balance computational efficiency with registration accuracy.</p> <p>Main Processing Pipeline:</p> <ol> <li>Data Import \u2013 Import and organize serial section data using <code>FlowImport</code>.</li> <li>Image Alignment \u2013 Multi-stage registration process to align neighboring sections using <code>AutoFlowMultiCenter2/3</code>.</li> <li>3D Reconstruction \u2013 Build 3D tissue models from aligned sections using <code>TransformDB</code>.</li> <li>Spatial Analysis \u2013 Analyze spatial transcriptomics data in the 3D context using <code>FlowExport</code>.</li> </ol>"},{"location":"examples/usage/#image-alignment-workflow","title":"Image Alignment Workflow","text":"<p>The image alignment workflow is a core component of SpaceMap, combining coarse affine registration with fine LDDMM registration to accurately align tissue sections. This two-stage approach preserves both global structure and local micro-anatomy.</p> <p>Image Alignment Pipeline:</p> <ol> <li>Data Import: Raw data (cell coordinates and optional images) is imported using <code>FlowImport.init_xys()</code>, which organizes the data for processing.</li> <li>Slice Creation: The imported data is organized into <code>Slice</code> objects, which serve as the basic data structure for the alignment process.</li> <li>Coarse Alignment:</li> <li>Affine Registration: Initial alignment using <code>AutoFlowMultiCenter2.affine()</code> to handle large displacements between sections.</li> <li>Feature Matching: Extraction of corresponding features between sections using SIFT or LOFTR for improved alignment.</li> <li>Fine Alignment:</li> <li>LDDMM Registration: Local non-linear alignment using <code>AutoFlowMultiCenter2.ldm_pair()</code> or <code>AutoFlowMultiCenter2DF.ldm_pair()</code>.</li> <li>Grid Generation: Creation of transformation grids using <code>LDDMMRegistration.generate_img_grid()</code>.</li> <li>Grid Merging: Combination of transformation grids using <code>ldm_merge_pair()</code> for global consistency.</li> </ol> <p>The <code>AutoFlowMultiCenter2DF</code> class implements a multi-center approach that handles bidirectional alignment from a central slice, improving global consistency in the alignment.</p>"},{"location":"examples/usage/#3d-reconstruction-workflow","title":"3D Reconstruction Workflow","text":"<p>The 3D reconstruction workflow transforms aligned 2D sections into a coherent 3D model by computing transformation grids and applying them to the original data.</p> <p>3D Reconstruction Pipeline:</p> <ol> <li>Grid Processing:</li> <li>Grid Saving: Each alignment transformation is saved as a grid using <code>Slice.data.saveGrid()</code>.</li> <li>TransformDB Creation: A <code>TransformDB</code> object is created to store and manage all transformation grids.</li> <li>Grid Application: Transformation grids are applied to sections with <code>_apply_grid()</code>.</li> <li>3D Model Building:</li> <li>Point Transformation: Cell coordinates are transformed using <code>TransformDB.apply_points()</code>.</li> <li>Model Export: The 3D model is exported with <code>FlowExport.export_imgs()</code> for visualization and analysis.</li> </ol> <p>The grid merging process is a key component of the 3D reconstruction workflow, implemented in <code>AutoFlowMultiCenter2DF.ldm_merge_pair()</code>. This method ensures that transformation grids are properly merged to maintain global consistency in the 3D reconstruction.</p>"},{"location":"examples/usage/#spatial-transcriptomics-analysis-workflow","title":"Spatial Transcriptomics Analysis Workflow","text":"<p>The spatial transcriptomics analysis workflow enables analysis of gene expression patterns in the 3D reconstructed tissue.</p> <p>Spatial Analysis Pipeline:</p> <ol> <li>Data Analysis:</li> <li>Cell Matrix Analysis: Processing of cell-gene expression matrices within the 3D context.</li> <li>Gene Expression Analysis: Analysis of gene expression patterns across the 3D tissue.</li> <li>Cell Type Identification: Classification of cells based on gene expression profiles.</li> <li>Visualization &amp; Export:</li> <li>Spatial Statistics: Calculation of spatial relationships and patterns among cell types.</li> <li>3D Visualization: Interactive visualization of cells with gene expression data in 3D space.</li> </ol> <p>The <code>FlowExport</code> class handles the export of 3D models and transformed cell coordinates for downstream analysis.</p>"},{"location":"examples/usage/#multi-center-workflow-strategy","title":"Multi-center Workflow Strategy","text":"<p>SpaceMap implements special multi-center workflows through the <code>AutoFlowMultiCenter2DF</code> and <code>AutoFlowMultiCenter3</code> classes. These workflows start from a central slice and work outward in both directions, which helps maintain global consistency in the reconstruction.</p> <p>Multi-center Alignment Strategy:</p> <ol> <li>Center Slice Selection: The middle slice is selected as a reference point.</li> <li>Bidirectional Processing: Slices are processed in both forward and backward directions from the center.</li> <li>Parallel Computation: Slice pairs can be processed in parallel for efficiency.</li> <li>Grid Merging: Transformation grids are merged to ensure global consistency.</li> </ol> <p>This bidirectional processing strategy helps maintain the global structure of the tissue while allowing for local deformations.</p>"},{"location":"examples/usage/#configuration-options-and-parameters","title":"Configuration Options and Parameters","text":"<p>The SpaceMap workflows can be customized through various configuration options and parameters:</p> Parameter Description Default Component alignMethod Method for initial alignment \"auto\" AutoFlowMultiCenter2 gpu GPU device to use for computation None AutoFlowMultiCenter2 finalErr Error threshold for LDDMM convergence System default ldm_pair centerTrain Whether to use center-based training False ldm_pair show Whether to display visual results False Various methods saveGridKey Key for saving transformation grids pairGridKey ldm_pair <p>These parameters allow fine-tuning of the workflows for different types of data and reconstruction requirements.</p>"},{"location":"examples/usage/#programming-against-the-workflow-system","title":"Programming Against the Workflow System","text":"<p>When developing with the SpaceMap framework, it's important to understand the main workflow components and how they work together. The following best practices are recommended:</p> <ol> <li>Initialize with FlowImport: Always start by creating a <code>FlowImport</code> object to organize your data.</li> <li>Use the appropriate AutoFlow class: Select between <code>AutoFlowMultiCenter2</code>, <code>AutoFlowMultiCenter2DF</code>, or <code>AutoFlowMultiCenter3</code> depending on your alignment needs.</li> <li>Follow the workflow steps sequentially: First perform affine alignment, then LDDMM, then merge the grids.</li> <li>Use TransformDB for transformation management: Store all transformations in a <code>TransformDB</code> for efficient application.</li> <li>Export results with FlowExport: Use <code>FlowExport</code> to save your 3D models and transformed data.</li> </ol>"},{"location":"examples/usage/#detailed-usage-guide","title":"Detailed Usage Guide","text":"<p>This guide provides detailed examples for using Space-map's core functionalities. Space-map employs a sophisticated two-stage registration approach that combines dimensionality reduction with robust feature matching to handle large-scale single-cell data efficiently.</p>"},{"location":"examples/usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Data Preparation</li> <li>Registration Workflow</li> <li>Advanced Registration</li> <li>Working with Large Datasets</li> <li>Visualizing Results</li> <li>Exporting 3D Models</li> </ul>"},{"location":"examples/usage/#data-preparation","title":"Data Preparation","text":""},{"location":"examples/usage/#supported-data-formats","title":"Supported Data Formats","text":"<p>Space-map works with various data types:</p> <pre><code>import spacemap as sm\nimport numpy as np\nimport pandas as pd\nimport tifffile\n\n# Cell coordinates (numpy array)\npoints = np.array([[x1, y1], [x2, y2], ...])  # Shape: (n_cells, 2)\n\n# Cell coordinates with metadata (pandas DataFrame)\ncells_df = pd.DataFrame({\n    'x': [x1, x2, ...],\n    'y': [y1, y2, ...],\n    'cell_type': ['T-cell', 'B-cell', ...],\n    'gene_expression': [0.5, 0.7, ...]\n})\n\n# Image data (numpy array)\nimg = tifffile.imread('section_image.tif')  # Shape: (height, width) or (height, width, channels)\n</code></pre>"},{"location":"examples/usage/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code># Filter cells by type\nt_cells = cells_df[cells_df['cell_type'] == 'T-cell']\nt_cell_coords = t_cells[['x', 'y']].values\n\n# Normalize coordinates\nnormalized_points = sm.utils.compute.normalize_points(points)\n\n# Process image data\nprocessed_img = sm.utils.img.preprocess_image(img, contrast_enhance=True, denoise=True)\n</code></pre>"},{"location":"examples/usage/#registration-workflow","title":"Registration Workflow","text":""},{"location":"examples/usage/#full-registration-pipeline","title":"Full Registration Pipeline","text":"<pre><code># Load data for consecutive sections\nsections_data = []\nfor i in range(5):  # 5 sections\n    # Load points and images\n    points = np.load(f\"section{i}_points.npy\")\n    img = tifffile.imread(f\"section{i}_image.tif\")\n    sections_data.append({\"points\": points, \"image\": img})\n\n# Initialize transformation database\ntransform_db = sm.flow.outputs.TransformDB()\n\n# Register consecutive sections\nfor i in range(len(sections_data) - 1):\n    source = sections_data[i]\n    target = sections_data[i+1]\n\n    # Step 1: Affine registration\n    affine_transform = sm.registration.affine_register(\n        source_points=source[\"points\"],\n        target_points=target[\"points\"]\n    )\n\n    # Apply affine transformation\n    source_points_affine = sm.affine.transform_points(source[\"points\"], affine_transform)\n\n    # Step 2: Feature matching (optional, for improved alignment)\n    keypoints_source, keypoints_target, matches = sm.matches.find_matches(\n        source[\"image\"], target[\"image\"], method=\"loftr\"\n    )\n\n    # Step 3: Fine registration with flow\n    flow_transform = sm.flow.register(\n        source_points=source_points_affine,\n        target_points=target[\"points\"],\n        source_keypoints=keypoints_source,\n        target_keypoints=keypoints_target,\n        matches=matches,\n        iterations=200\n    )\n\n    # Store transforms in database\n    transform_db.add_transform(\n        source_id=i,\n        target_id=i+1,\n        affine_transform=affine_transform,\n        flow_transform=flow_transform\n    )\n\n# Save transformation database\ntransform_db.save(\"registration_results.pkl\")\n</code></pre>"},{"location":"examples/usage/#advanced-registration","title":"Advanced Registration","text":""},{"location":"examples/usage/#multi-modal-registration","title":"Multi-modal Registration","text":"<pre><code># Register using both point coordinates and image features\nresult = sm.registration.multimodal_register(\n    source_points=source_points,\n    target_points=target_points,\n    source_image=source_image,\n    target_image=target_image,\n    point_weight=0.7,  # Weight for point-based alignment\n    image_weight=0.3   # Weight for image-based alignment\n)\n</code></pre>"},{"location":"examples/usage/#global-consistency-optimization","title":"Global Consistency Optimization","text":"<pre><code># Load transformation database\ntransform_db = sm.flow.outputs.TransformDB.load(\"registration_results.pkl\")\n\n# Optimize for global consistency\noptimized_transforms = sm.registration.global_optimize(transform_db)\n\n# Apply optimized transformations to all sections\naligned_sections = []\nfor i in range(len(sections_data)):\n    if i == 0:  # Reference section\n        aligned_sections.append(sections_data[0][\"points\"])\n    else:\n        # Apply sequence of transformations to align to reference\n        aligned_points = sm.registration.apply_transform_sequence(\n            sections_data[i][\"points\"],\n            optimized_transforms,\n            source_id=i,\n            target_id=0\n        )\n        aligned_sections.append(aligned_points)\n</code></pre>"},{"location":"examples/usage/#working-with-large-datasets","title":"Working with Large Datasets","text":""},{"location":"examples/usage/#chunked-processing","title":"Chunked Processing","text":"<pre><code># Process large point clouds in chunks\nchunk_size = 10000  # Number of points per chunk\nnum_chunks = len(large_point_set) // chunk_size + 1\n\ntransformed_points = []\nfor i in range(num_chunks):\n    start_idx = i * chunk_size\n    end_idx = min((i + 1) * chunk_size, len(large_point_set))\n\n    chunk = large_point_set[start_idx:end_idx]\n    transformed_chunk = sm.flow.transform_points(chunk, flow_transform)\n    transformed_points.append(transformed_chunk)\n\n# Combine all transformed chunks\ntransformed_points = np.vstack(transformed_points)\n</code></pre>"},{"location":"examples/usage/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Enable GPU acceleration\nsm.base.root.DEVICE = 0  # Use first GPU\n\n# Perform registration with GPU acceleration\nflow_transform = sm.flow.register(\n    source_points=source_points,\n    target_points=target_points,\n    use_gpu=True\n)\n</code></pre>"},{"location":"examples/usage/#visualizing-results","title":"Visualizing Results","text":""},{"location":"examples/usage/#interactive-visualization","title":"Interactive Visualization","text":"<pre><code># Compare before/after registration\nsm.utils.compare.interactive_comparison(\n    source_points=source_points,\n    target_points=target_points,\n    aligned_points=transformed_points\n)\n\n# Visualize transformation field\nsm.utils.show.plot_transformation_field(flow_transform)\n\n# Interactive 3D model visualization\nsm.utils.show.interactive_3d_plot(model_3d)\n</code></pre>"},{"location":"examples/usage/#custom-visualization","title":"Custom Visualization","text":"<pre><code># Create custom visualization with cell types\ncell_types = ['T-cell', 'B-cell', 'Epithelial', 'Stromal']\ncell_colors = ['red', 'blue', 'green', 'purple']\n\n# Plot cells colored by type\nplt.figure(figsize=(10, 8))\nfor cell_type, color in zip(cell_types, cell_colors):\n    mask = cells_df['cell_type'] == cell_type\n    plt.scatter(\n        cells_df.loc[mask, 'x'], \n        cells_df.loc[mask, 'y'],\n        c=color,\n        label=cell_type,\n        s=3,\n        alpha=0.7\n    )\nplt.legend()\nplt.title('Cells by Type')\nplt.show()\n</code></pre>"},{"location":"examples/usage/#exporting-3d-models","title":"Exporting 3D Models","text":""},{"location":"examples/usage/#exporting-to-common-formats","title":"Exporting to Common Formats","text":"<pre><code># Export 3D model to OBJ format\nsm.utils.output.export_to_obj(model_3d, \"tissue_model.obj\")\n\n# Export as point cloud\nsm.utils.output.export_point_cloud(model_3d, \"tissue_model.ply\")\n\n# Export to Imaris format\nsm.utils.imaris.export_points(model_3d, \"tissue_model.ims\", point_size=5)\n</code></pre>"},{"location":"examples/usage/#saving-results-for-analysis","title":"Saving Results for Analysis","text":"<pre><code># Save aligned data for further analysis\nnp.save(\"aligned_sections.npy\", aligned_sections)\n\n# Export as CSV with metadata\naligned_df = pd.DataFrame({\n    'x': model_3d[:, 0],\n    'y': model_3d[:, 1],\n    'z': model_3d[:, 2],\n    'section': section_indices,\n    'cell_type': cell_types\n})\naligned_df.to_csv(\"aligned_model.csv\", index=False)\n</code></pre>"},{"location":"examples/usage/#advanced-topics","title":"Advanced Topics","text":"<p>For more advanced usage, please refer to:</p> <ul> <li>API Reference for detailed function documentation</li> <li>Examples for specific use cases</li> <li>Individual module documentation for specialized features </li> </ul>"},{"location":"feature/feature-matching/","title":"Feature Matching","text":"<p>Related source files: - affine/AutoGrad.py - affine_block/LdmAffine.py - base/flowBase.py - flow/flowImport.py - matches/LOFTR.py - matches/LOFTR2.py - matches/matchShow.py - matches/siftFind.py - registration/ldm/torch_LDDMMBase.py</p>"},{"location":"feature/feature-matching/#introduction","title":"Introduction","text":"<p>Feature matching is a key component in the SpaceMap framework, used to identify corresponding points between images for accurate registration of tissue sections. SpaceMap implements two main feature matching methods: SIFT (Scale-Invariant Feature Transform) and LOFTR (Local Feature Transformer).</p> <p>For more information about the registration workflow, see: Registration System (LDDMM) and Image Alignment.</p>"},{"location":"feature/feature-matching/#overview","title":"Overview","text":"<p>The feature matching system is mainly responsible for: 1. Detecting salient features in source and target images 2. Finding correspondences between these features 3. Filtering outlier matches 4. Providing matches to the registration algorithm for alignment</p>"},{"location":"feature/feature-matching/#feature-matching-components","title":"Feature Matching Components","text":"<p>SpaceMap implements two main feature matching methods: - SIFT (traditional computer vision method) - LOFTR (deep learning-based feature matching)</p>"},{"location":"feature/feature-matching/#sift-matching","title":"SIFT Matching","text":"<p>SIFT (Scale-Invariant Feature Transform) is a traditional local feature detection and description algorithm. In SpaceMap, SIFT is the default feature matching method.</p>"},{"location":"feature/feature-matching/#key-functions","title":"Key Functions","text":"<ol> <li>Feature Detection: <code>__sift_kp()</code> detects keypoints and computes descriptors.</li> <li>Feature Matching: <code>siftImageAlignment()</code> matches features between images, applies ratio test to filter matches, and returns corresponding point lists.</li> </ol>"},{"location":"feature/feature-matching/#implementation-details","title":"Implementation Details","text":"<ul> <li>Uses OpenCV's <code>SIFT_create()</code> for keypoint detection</li> <li>Uses BFMatcher for descriptor matching</li> <li>Applies ratio test (typically 0.75) to filter low-quality matches</li> <li>Matches are sorted by quality (inverse distance)</li> <li>Supports multiple detectors such as SURF, ORB, etc.</li> </ul>"},{"location":"feature/feature-matching/#loftr-matching","title":"LOFTR Matching","text":"<p>LOFTR (Local Feature Transformer) is a deep learning-based feature matching method based on Transformer, directly predicting matches from image pairs.</p>"},{"location":"feature/feature-matching/#key-functions_1","title":"Key Functions","text":"<ol> <li>Neural Network Matching: <code>loftr_compute_matches()</code> uses a pre-trained Transformer model to directly predict matches between images.</li> <li>Multi-channel Support: The <code>LOFTR</code> class supports multi-channel image matching.</li> </ol>"},{"location":"feature/feature-matching/#implementation-details_1","title":"Implementation Details","text":"<ul> <li>Based on Kornia's <code>LoFTR</code> implementation</li> <li>Supports indoor/outdoor scene models (<code>method</code> parameter)</li> <li>Input is automatically normalized to [0, 1]</li> <li>Matches include confidence scores</li> <li>Supports confidence threshold or fixed number of matches</li> <li><code>LOFTR2</code> supports image block (quadrant) matching</li> </ul>"},{"location":"feature/feature-matching/#integration-with-registration-workflow","title":"Integration with Registration Workflow","text":"<p>Feature matching is a key step in the image alignment workflow, providing the basis for affine and nonlinear registration.</p>"},{"location":"feature/feature-matching/#homography-estimation","title":"Homography Estimation","text":"<p>After matches are determined, SpaceMap uses RANSAC to estimate the homography transformation for image alignment: - Main function: <code>createHFromPoints2()</code> - Input: matched points - Uses OpenCV's <code>findHomography()</code> + RANSAC to compute the transformation - Applies the transformation and checks alignment quality - Fine-tunes to minimize alignment error</p>"},{"location":"feature/feature-matching/#match-visualization","title":"Match Visualization","text":"<p>SpaceMap provides match visualization tools for quality control of alignment.</p>"},{"location":"feature/feature-matching/#matchshow-class","title":"MatchShow Class","text":"<p>The <code>MatchShow</code> class inherits from <code>AffineBlock</code> and is used for feature match visualization: - Displays original points in the first image - Displays transformed points in the second image - Connects matched points with lines</p>"},{"location":"feature/feature-matching/#application-in-autoflow-workflow","title":"Application in AutoFlow Workflow","text":"<p>Feature matching is mainly used in the AutoFlow automatic registration workflow: 1. <code>FlowImport</code> loads data and creates slices 2. Feature matching is used to find correspondences between adjacent slices 3. These correspondences guide affine registration for coarse alignment 4. LDDMM further refines nonlinear transformations</p>"},{"location":"feature/feature-matching/#best-practices","title":"Best Practices","text":"Task Recommended Method Main Parameters High-contrast tissue SIFT <code>matchr=0.75</code>, method='sift' Low-contrast images LOFTR model=\"indoor\" Multi-channel data LOFTR (multiChannel) select strongest channel or mean Large images LOFTR2 (block matching) quadrant partition Auto threshold <code>autoSetMatchr()</code> minCount parameter"},{"location":"feature/feature-matching/#key-parameters","title":"Key Parameters","text":"Parameter Description Typical Value matchr SIFT ratio threshold/LOFTR confidence 0.75 (SIFT), 0.8-0.95 (LOFTR) method Feature detection method 'sift', 'orb', 'surf' scale Whether to normalize images True (normalize) device GPU device for LOFTR CUDA device number"},{"location":"feature/feature-matching/#conclusion","title":"Conclusion","text":"<p>SpaceMap's feature matching system combines traditional computer vision (SIFT) and deep learning (LOFTR) methods, providing flexible solutions for different data types and registration challenges.</p> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"grid/grid-transformations/","title":"Grid Transformations","text":"<p>Related source files: - utils/fig.py - utils/grid.py - utils/grid3.py - utils/grid_points.py - utils/imaris.py - utils/output.py</p>"},{"location":"grid/grid-transformations/#introduction","title":"Introduction","text":"<p>Grid Transformation is a core mechanism in the SpaceMap framework for representing and applying nonlinear spatial transformations. It supports mapping points (such as cell coordinates) from original section space to registered 3D space, serving as the primary carrier structure for LDDMM registration results.</p> <p>For more about LDDMM registration, see: Registration System (LDDMM).</p>"},{"location":"grid/grid-transformations/#basic-concepts-of-grid-transformations","title":"Basic Concepts of Grid Transformations","text":""},{"location":"grid/grid-transformations/#what-is-a-transformation-grid","title":"What is a Transformation Grid?","text":"<p>A transformation grid is a regular 2D array of displacement vectors that defines how each point in the source space maps to the target space. Each grid point contains an (x, y) vector for the target coordinates.</p> <p>In SpaceMap, grids are represented as 3D tensors with shape <code>[N, N, 2]</code>, where N is the grid resolution and the last dimension represents (x, y) coordinates.</p>"},{"location":"grid/grid-transformations/#coordinate-systems","title":"Coordinate Systems","text":"<ul> <li>Grid Coordinates: Normalized to [-1, 1]</li> <li>Image/Point Coordinates: Pixel or data coordinates, typically [0, N*xyd]</li> <li>The <code>xyd</code> (XY density) parameter defines the scaling relationship between them</li> </ul>"},{"location":"grid/grid-transformations/#core-operations","title":"Core Operations","text":""},{"location":"grid/grid-transformations/#grid-sampling-and-bilinear-interpolation","title":"Grid Sampling and Bilinear Interpolation","text":"<p>The core operation is sampling points through the grid (<code>grid_sample_points_vectorized</code>), implementing smooth point transformation: 1. Locate the grid cell containing the point 2. Calculate interpolation ratios 3. Perform bilinear interpolation using corner vectors 4. Scale the result to target coordinate system</p> <p>Interpolation Formula: <pre><code>result = (1-y_ratio) * [(1-x_ratio)*top_left + x_ratio*top_right] + \n         y_ratio * [(1-x_ratio)*bottom_left + x_ratio*bottom_right]\n</code></pre></p>"},{"location":"grid/grid-transformations/#inverse-grid-generation","title":"Inverse Grid Generation","text":"<p>Inverse grids (<code>inverse_grid_train</code>) are obtained by minimizing the error between forward and inverse transformations, supporting bidirectional point mapping.</p>"},{"location":"grid/grid-transformations/#grid-generation-from-point-sets","title":"Grid Generation from Point Sets","text":"<p><code>points_gen_grid_train</code> can optimize and generate smooth grids based on point pair relationships, implementing nonlinear mapping between arbitrary point sets.</p>"},{"location":"grid/grid-transformations/#grid-merging","title":"Grid Merging","text":"<p><code>merge_grid_img</code> supports sequential composition of multiple grid transformations, facilitating complex workflow chaining.</p>"},{"location":"grid/grid-transformations/#main-classes-and-interfaces","title":"Main Classes and Interfaces","text":"<ul> <li>GridGenerate: Object-oriented interface supporting grid generation from point sets, grid refinement, and grid transformation application.</li> <li>ModelGenerate: Used for 3D model generation, applying grid transformations to cell boundaries and coordinates for registered 3D visualization.</li> </ul>"},{"location":"grid/grid-transformations/#performance-and-parameters","title":"Performance and Parameters","text":"<ul> <li>Grid resolution N determines precision and memory consumption</li> <li><code>xyd</code> controls scaling between grid and image space</li> <li>Vectorized operations efficiently handle large-scale point sets</li> <li>Bilinear interpolation balances speed and accuracy</li> <li>Inverse grid computation is more time-consuming than forward transformation</li> </ul>"},{"location":"grid/grid-transformations/#integration-with-spacemap-workflow","title":"Integration with SpaceMap Workflow","text":"<ul> <li>Grid transformations are fundamental to LDDMM registration, point set registration, and 3D reconstruction workflows</li> <li>All grid transformations between sections are uniformly managed through <code>TransformDB</code></li> <li>Supports forward/inverse transformations, batch point set processing, and grid composition</li> </ul> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"overview/changelog/","title":"Changelog","text":"<p>All notable changes to SpaceMap will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"overview/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"overview/changelog/#added","title":"Added","text":"<ul> <li>Initial documentation website</li> <li>Improved API documentation</li> </ul>"},{"location":"overview/changelog/#010-yyyy-mm-dd","title":"[0.1.0] - YYYY-MM-DD","text":""},{"location":"overview/changelog/#added_1","title":"Added","text":"<ul> <li>Initial release of SpaceMap</li> <li>Basic registration functionality</li> <li>Affine registration for coarse alignment</li> <li>Flow-based fine registration with LDDMM</li> <li>Feature matching using SIFT and LoFTR</li> <li>Utils for visualization and data processing</li> <li>Support for multi-modal data integration</li> <li>GPU acceleration for transform computations</li> </ul>"},{"location":"overview/changelog/#known-issues","title":"Known Issues","text":"<ul> <li>Large datasets (&gt;10M points) might require significant memory resources</li> <li>Some visualization functions may be slow with very large datasets </li> </ul>"},{"location":"overview/contributing/","title":"Contributing to SpaceMap","text":"<p>Thank you for your interest in contributing to SpaceMap! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"overview/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and considerate of others when contributing to SpaceMap. We aim to foster an inclusive and welcoming community.</p>"},{"location":"overview/contributing/#how-to-contribute","title":"How to Contribute","text":"<p>There are many ways to contribute to SpaceMap:</p> <ol> <li>Reporting bugs</li> <li>Suggesting enhancements</li> <li>Writing documentation</li> <li>Submitting code changes</li> <li>Reviewing pull requests</li> </ol>"},{"location":"overview/contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you find a bug in SpaceMap, please report it by opening an issue on GitHub. When reporting bugs, please include:</p> <ul> <li>A clear and descriptive title</li> <li>A detailed description of the bug</li> <li>Steps to reproduce the bug</li> <li>Expected behavior</li> <li>Actual behavior</li> <li>Screenshots or code snippets (if applicable)</li> <li>Your environment (Python version, operating system, etc.)</li> </ul>"},{"location":"overview/contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>We welcome suggestions for enhancing SpaceMap. To suggest an enhancement, please open an issue on GitHub with:</p> <ul> <li>A clear and descriptive title</li> <li>A detailed description of the enhancement</li> <li>Any relevant examples or use cases</li> <li>Any implementation ideas you may have</li> </ul>"},{"location":"overview/contributing/#contributing-code","title":"Contributing Code","text":"<ol> <li>Fork the repository</li> <li>Create a new branch for your changes (<code>git checkout -b feature/your-feature-name</code>)</li> <li>Make your changes</li> <li>Add or update tests as necessary</li> <li>Run the tests to ensure they pass</li> <li>Commit your changes (<code>git commit -am 'Add your feature'</code>)</li> <li>Push to your branch (<code>git push origin feature/your-feature-name</code>)</li> <li>Create a new pull request</li> </ol>"},{"location":"overview/contributing/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Write docstrings for all functions, classes, and modules</li> <li>Add type hints where appropriate</li> <li>Ensure all tests pass</li> <li>Add new tests for new functionality</li> </ul>"},{"location":"overview/contributing/#writing-documentation","title":"Writing Documentation","text":"<p>Good documentation is crucial for the usability of SpaceMap. You can contribute by:</p> <ul> <li>Improving existing documentation</li> <li>Adding examples</li> <li>Fixing typos or errors</li> <li>Adding tutorials or guides</li> </ul>"},{"location":"overview/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/a12910/spacemap.git\ncd spacemap\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Run tests:    <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"overview/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all tests pass and the code meets our coding standards</li> <li>Update the documentation as necessary</li> <li>Update the README.md with details of changes if appropriate</li> <li>The pull request will be reviewed by at least one maintainer</li> <li>Once approved, the pull request will be merged</li> </ol>"},{"location":"overview/contributing/#license","title":"License","text":"<p>By contributing to SpaceMap, you agree that your contributions will be licensed under the same license as the project.</p> <p>Thank you for your contributions to SpaceMap! </p>"},{"location":"overview/installation/","title":"Installation Guide","text":"<p>There are several ways to install SpaceMap. Choose the method that best fits your needs.</p>"},{"location":"overview/installation/#installing-via-pip-recommended","title":"Installing via pip (Recommended)","text":"<p>The simplest way to install SpaceMap is via pip:</p> <pre><code>pip install spacemap\n</code></pre> <p>This will install SpaceMap and all its dependencies automatically.</p>"},{"location":"overview/installation/#installing-from-source","title":"Installing from Source","text":"<p>For the latest features or development purposes, you can install SpaceMap directly from the GitHub repository:</p> <pre><code>git clone https://github.com/a12910/spacemap.git\ncd spacemap\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in \"editable\" mode, which means changes to the source code will be reflected in your environment without reinstallation.</p>"},{"location":"overview/installation/#dependencies","title":"Dependencies","text":"<p>SpaceMap requires the following Python packages:</p> <pre><code>opencv-python\npandas\nnumpy\ntorch\nkornia\nscikit-learn\ntifffile\nmatplotlib\nnumba\nscipy\ntqdm\nnibabel\nseaborn\nscikit-image\n</code></pre> <p>These dependencies will be automatically installed when installing via pip. If you're installing manually, you can install all required dependencies with:</p> <pre><code>pip install -r requirement.txt\n</code></pre>"},{"location":"overview/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For optimal performance, we recommend the following:</p>"},{"location":"overview/installation/#gpu-support","title":"GPU Support","text":"<p>SpaceMap can leverage GPU acceleration for faster processing:</p> <ol> <li>Make sure you have a CUDA-compatible NVIDIA GPU</li> <li>Install the appropriate CUDA toolkit version compatible with your PyTorch version</li> <li>Install the GPU version of PyTorch:</li> </ol> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>Replace <code>cu118</code> with your CUDA version as needed.</p>"},{"location":"overview/installation/#verifying-installation","title":"Verifying Installation","text":"<p>You can verify your installation by importing SpaceMap in Python:</p> <pre><code>import spacemap as sm\nprint(sm.__version__)\n</code></pre> <p>If the installation was successful, this will print the version number without any errors.</p>"},{"location":"overview/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during installation:</p> <ol> <li>Ensure your Python version is 3.7 or higher</li> <li>Update pip: <code>pip install --upgrade pip</code></li> <li>Check for conflicts with other installed packages</li> <li>Try installing in a fresh virtual environment:</li> </ol> <pre><code>python -m venv spacemap_env\nsource spacemap_env/bin/activate  # On Windows: spacemap_env\\Scripts\\activate\npip install spacemap\n</code></pre> <p>For more help, please open an issue on GitHub. </p>"},{"location":"overview/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with SpaceMap by walking through a basic example.</p> <p></p>"},{"location":"overview/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have:</p> <ol> <li>Installed SpaceMap (see Installation Guide)</li> <li>Prepared your spatial data (e.g., cell coordinates, images)</li> </ol>"},{"location":"overview/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"overview/quickstart/#importing-spacemap","title":"Importing SpaceMap","text":"<pre><code>import spacemap\nfrom spacemap import Slice\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"overview/quickstart/#loading-and-processing-cell-coordinate-data","title":"Loading and Processing Cell Coordinate Data","text":"<pre><code># Load cell coordinate data from CSV file\n# CSV file should contain cell coordinates and layer information\ndf = pd.read_csv(\"path/to/cells.csv.gz\")\ngroups = df.groupby(\"layer\")\n\n# Organize data by layers\nxys = []  # xy coordinates (list of N*2 numpy arrays)\nids = []  # layer IDs\n\nfor layer, dff in groups:\n    xy = dff[[\"x\", \"y\"]].values\n    ids.append(layer)\n    xys.append(xy)\n\n# Set project base folder for storing all data\nbase = \"data/flow\"\n\n# Initialize flow processing\nflow = spacemap.flow.FlowImport(base)\n\n# Initialize with xy coordinates and layer IDs\nflow.init_xys(xys, ids)\n\n# Get slice objects\nslices = flow.slices\n</code></pre>"},{"location":"overview/quickstart/#performing-affine-registration","title":"Performing Affine Registration","text":"<p>SpaceMap uses a two-stage registration approach. First, we perform affine registration for coarse alignment:</p> <pre><code># Create registration manager\nmgr = spacemap.flow.AutoFlowMultiCenter3(slices)\n\n# Set alignment method\nmgr.alignMethod = \"auto\"\n\n# Perform affine registration\n# The \"DF\" parameter specifies the method\n# show=True will display visualization of the registration process\nmgr.affine(\"DF\", show=True)\n</code></pre>"},{"location":"overview/quickstart/#fine-registration-with-lddmm","title":"Fine Registration with LDDMM","text":"<p>After affine registration, we can perform fine registration using LDDMM:</p> <pre><code># Perform LDDMM registration between adjacent sections\n# This provides high-precision alignment while preserving local structures\nmgr.ldm_pair(Slice.align1Key, Slice.align2Key, show=True)\n</code></pre>"},{"location":"overview/quickstart/#exporting-results","title":"Exporting Results","text":"<pre><code># Export registration results\nexport = spacemap.flow.FlowExport(slices)\nimgs = export.export_imgs()\npoints = export.\n\n# Export can be used to save transformed coordinates\n# export.points_csv(\"aligned_cells.csv\")\n</code></pre>"},{"location":"overview/quickstart/#next-steps","title":"Next Steps","text":"<p>After completing this quick start guide, you can explore more advanced features:</p> <ul> <li>Detailed Usage Guide for more complex examples</li> <li>API Reference for comprehensive documentation</li> <li>Examples for specific use cases</li> </ul> <p>For any issues or questions, please open an issue on GitHub. </p>"},{"location":"reconstruction/3d-reconstruction/","title":"3D Reconstruction","text":"<p>\u76f8\u5173\u6e90\u6587\u4ef6\uff1a - affine/FilterGraphPart.py - flow/init.py - flow/afFlow2Basic.py - flow/afFlow2MultiDF.py - matches/init.py - matches/matchInitMulti.py - registration/ldm/torch_LDDMM2D.py - utils/AutoGradHE.py - utils/model3d.py</p>"},{"location":"reconstruction/3d-reconstruction/#_1","title":"\u7b80\u4ecb","text":"<p>SpaceMap \u7684 3D \u91cd\u5efa\u7cfb\u7edf\u5c06\u4e00\u7cfb\u5217\u5df2\u914d\u51c6\u7684 2D \u7ec4\u7ec7\u5207\u7247\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684 3D \u6a21\u578b\u3002</p> <ul> <li>\u5148\u5bf9\u6bcf\u4e2a\u5207\u7247\u751f\u6210\u5750\u6807\u53d8\u6362 grid</li> <li>\u5e94\u7528\u53d8\u6362\u5c06\u5207\u7247\u6570\u636e\u6620\u5c04\u5230\u7edf\u4e00 3D \u7a7a\u95f4</li> <li>\u5408\u5e76\u6240\u6709\u5207\u7247\u6570\u636e\uff0c\u5f62\u6210\u5b8c\u6574 3D \u7ec4\u7ec7\u7ed3\u6784</li> </ul> <p>For more about image alignment, see: Image Alignment.</p>"},{"location":"reconstruction/3d-reconstruction/#grid","title":"Grid \u751f\u6210","text":"<ul> <li>\u901a\u8fc7 <code>model3d.generate_grid</code> \u4e3a\u6bcf\u5c42\u751f\u6210\u7a20\u5bc6\u53d8\u6362\u573a\uff08grid\uff09\uff0c\u5b9e\u73b0\u539f\u59cb\u4e0e\u914d\u51c6\u7a7a\u95f4\u7684\u7cbe\u786e\u6620\u5c04</li> <li>\u652f\u6301\u5e76\u884c\u5904\u7406\u63d0\u5347\u6548\u7387</li> <li>grid \u4ee5 numpy \u6570\u7ec4\u4fdd\u5b58\uff0c\u4fbf\u4e8e\u540e\u7eed\u5e94\u7528</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - model3d.py#L18-L47</p>"},{"location":"reconstruction/3d-reconstruction/#_2","title":"\u53d8\u6362\u5e94\u7528","text":""},{"location":"reconstruction/3d-reconstruction/#_3","title":"\u56fe\u50cf\u53d8\u6362","text":"<ul> <li><code>grid_sample_img</code> \u5229\u7528 PyTorch grid sampling \u5bf9\u56fe\u50cf\u8fdb\u884c\u53cc\u7ebf\u6027\u63d2\u503c\u53d8\u6362</li> <li>\u652f\u6301\u9ad8\u6548 GPU \u52a0\u901f</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - model3d.py#L68-L104</p>"},{"location":"reconstruction/3d-reconstruction/#_4","title":"\u70b9\u96c6\u53d8\u6362","text":"<ul> <li>\u7ec6\u80de\u7b49\u70b9\u6570\u636e\u901a\u8fc7 AutoFlow \u6d41\u7a0b\u5148\u4eff\u5c04\u518d grid \u975e\u7ebf\u6027\u53d8\u6362</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - afFlow2Basic.py#L202-L210</p>"},{"location":"reconstruction/3d-reconstruction/#_5","title":"\u591a\u5206\u8fa8\u7387\u4e0e\u4e2d\u5fc3\u5207\u7247\u7b56\u7565","text":"<ul> <li>\u91c7\u7528\u591a\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u517c\u987e\u6548\u7387\u4e0e\u7cbe\u5ea6</li> <li>\u4ee5\u4e2d\u5fc3\u5207\u7247\u4e3a\u53c2\u8003\uff0c\u53cc\u5411\u914d\u51c6\uff0c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef</li> <li>\u652f\u6301\u5e76\u884c\u5904\u7406</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - afFlow2MultiDF.py#L62-L73</p>"},{"location":"reconstruction/3d-reconstruction/#ldm-pair","title":"LDM Pair \u914d\u51c6","text":"<ul> <li>\u90bb\u8fd1\u5207\u7247\u95f4\u91c7\u7528 LDDMM\uff08LDM Pair\uff09\u975e\u7ebf\u6027\u914d\u51c6\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6 grid</li> <li>\u7ed3\u679c\u7528\u4e8e\u540e\u7eed grid \u5408\u6210\u4e0e 3D \u91cd\u5efa</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - afFlow2MultiDF.py#L21-L60</p>"},{"location":"reconstruction/3d-reconstruction/#grid_1","title":"Grid \u5408\u6210","text":"<ul> <li>\u652f\u6301\u6b63\u5411\uff08raw\u2192aligned\uff09\u4e0e\u9006\u5411\uff08aligned\u2192raw\uff09grid \u5408\u6210\uff0c\u5f62\u6210\u5168\u5c40\u4e00\u81f4\u7684 3D \u5750\u6807\u7cfb</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - afFlow2Basic.py#L211-L223</p>"},{"location":"reconstruction/3d-reconstruction/#3d","title":"3D \u6a21\u578b\u5bfc\u51fa","text":"<ul> <li>\u53ef\u5bfc\u51fa\u6bcf\u5c42\u53d8\u6362\u540e\u56fe\u50cf\u3001grid\u3001\u70b9\u5750\u6807\u7b49</li> <li>\u4fbf\u4e8e\u53ef\u89c6\u5316\u4e0e\u540e\u7eed\u5206\u6790</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - flow/init.py#L1-L15 - model3d.py#L9-L17</p>"},{"location":"reconstruction/3d-reconstruction/#_6","title":"\u8bef\u5dee\u8bc4\u4f30","text":"<ul> <li>\u63d0\u4f9b\u5207\u7247\u95f4\u8bef\u5dee\u8bc4\u4f30\u5de5\u5177\uff0c\u91cf\u5316\u91cd\u5efa\u8d28\u91cf</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - model3d.py#L48-L62</p>"},{"location":"reconstruction/3d-reconstruction/#_7","title":"\u5168\u6d41\u7a0b\u96c6\u6210","text":"<ul> <li>3D \u91cd\u5efa\u6d41\u7a0b\u96c6\u6210\u4e86 grid \u751f\u6210\u3001\u53d8\u6362\u5e94\u7528\u3001\u914d\u51c6\u3001\u5408\u6210\u3001\u5bfc\u51fa\u4e0e\u8bef\u5dee\u8bc4\u4f30</li> <li>\u5173\u952e\u51fd\u6570\u5305\u62ec\uff1a<code>generate_imgs_basic</code>\u3001<code>generate_grid</code>\u3001<code>grid_sample_img</code>\u3001<code>ldm_pair</code>\u3001<code>ldm_merge_pair</code></li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - model3d.py#L9-L104 - afFlow2MultiDF.py#L21-L158 - registration/ldm/torch_LDDMM2D.py#L47-L63</p>"},{"location":"reconstruction/3d-reconstruction/#_8","title":"\u6027\u80fd\u4f18\u5316","text":"<ul> <li>\u5e76\u884c\u5904\u7406 grid \u751f\u6210</li> <li>GPU \u52a0\u901f\u53d8\u6362</li> <li>\u4e2d\u5fc3\u5207\u7247\u7b56\u7565\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef</li> </ul> <p>\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff1a - model3d.py#L32-L34 - afFlow2MultiDF.py#L120-L124 - registration/ldm/torch_LDDMM2D.py#L19-L22</p>"},{"location":"reconstruction/3d-reconstruction/#_9","title":"\u91cd\u8981\u7ec4\u4ef6\u8868","text":"\u7ec4\u4ef6 \u8bf4\u660e \u5b9e\u73b0\u4f4d\u7f6e GridGenerate \u751f\u6210\u53d8\u6362 grid model3d.py apply_grid \u5e94\u7528 grid \u5230\u5750\u6807 Slice \u7c7b _merge_grid grid \u5408\u6210 model3d.py grid_sample_img grid \u53ef\u89c6\u5316/\u56fe\u50cf\u53d8\u6362 model3d.py <p>SpaceMap \u7684 3D \u91cd\u5efa\u7cfb\u7edf\u901a\u8fc7\u9ad8\u6548\u7684 grid \u53d8\u6362\u3001LDDMM \u914d\u51c6\u3001\u591a\u5206\u8fa8\u7387\u4e0e\u4e2d\u5fc3\u5207\u7247\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u7684\u7ec4\u7ec7 3D \u91cd\u5efa\u3002 </p>"},{"location":"registration/registration-system-%28lddmm%29/","title":"Registration System (LDDMM)","text":"<p>Related source files: - registration/init.py - registration/lddmm.py - registration/lddmm2.py - registration/ldm2/torch_LDDMM2D.py - registration/ldm2/torch_LDDMMBase.py</p>"},{"location":"registration/registration-system-%28lddmm%29/#introduction","title":"Introduction","text":"<p>The LDDMM (Large Deformation Diffeomorphic Metric Mapping) registration system is a core component of the SpaceMap framework, responsible for precise nonlinear alignment of images and point clouds. It achieves high-precision registration between tissue sections by computing smooth, topology-preserving transformations.</p> <p>For more about grid transformation applications, see: Grid Transformations.</p>"},{"location":"registration/registration-system-%28lddmm%29/#overview","title":"Overview","text":"<p>The LDDMM registration system employs a multi-scale, two-stage registration strategy: 1. Affine Registration: Initial global linear transformation for coarse alignment 2. Diffeomorphic Registration: Smooth, invertible nonlinear transformation based on time-varying velocity fields for fine alignment</p> <p>This approach captures complex local deformations while preserving the topological structure of the original data.</p>"},{"location":"registration/registration-system-%28lddmm%29/#system-architecture","title":"System Architecture","text":"<p>The LDDMM registration system is primarily implemented through the <code>LDDMMRegistration</code> class, which inherits from the base <code>Registration</code> class and interfaces with the core LDDMM algorithm to complete mathematical optimization and transformation computation.</p>"},{"location":"registration/registration-system-%28lddmm%29/#registration-process","title":"Registration Process","text":"<p>The LDDMM registration process uses a hierarchical optimization strategy, aligning from coarse to fine to avoid local optima.</p>"},{"location":"registration/registration-system-%28lddmm%29/#registration-stages","title":"Registration Stages","text":"<ol> <li>Initial Affine Stage: High v_scale and epsilon for rough alignment</li> <li>Intermediate Affine Stage: Reduced parameters for improved accuracy</li> <li>Fine Affine Stage: Further reduced parameters for precise alignment</li> <li>LDDMM Stage: Switch to LDDMM mode for nonlinear transformation optimization</li> <li>Final LDDMM Stage: Minimum epsilon for highest precision</li> </ol>"},{"location":"registration/registration-system-%28lddmm%29/#key-parameters-table","title":"Key Parameters Table","text":"Stage Do Affine Do LDDMM v_scale epsilon niter 1 1 0 8.0 10000 300 2 1 0 4.0 1000 1000 3 1 0 1.0 50 6000 4 0 1 1.0 1000 20000 5 0 1 1.0 1 20000"},{"location":"registration/registration-system-%28lddmm%29/#implementation-details","title":"Implementation Details","text":""},{"location":"registration/registration-system-%28lddmm%29/#lddmmregistration-class","title":"LDDMMRegistration Class","text":"<ul> <li><code>run_affine()</code>: Execute affine registration only</li> <li><code>run()</code>: Complete registration process</li> <li><code>load_img(imgI, imgJ)</code>: Load registration images</li> <li><code>load_params()/output_params()</code>: Parameter serialization and deserialization</li> <li><code>apply_img(img)</code>: Apply transformation to new image</li> <li><code>generate_img_grid()</code>: Generate transformation grid for visualization</li> </ul>"},{"location":"registration/registration-system-%28lddmm%29/#transformation-structure","title":"Transformation Structure","text":"<ul> <li>Affine Part: 3\u00d73 matrix (affineA), containing rotation, scaling, and translation</li> <li>LDDMM Part: Time-varying velocity field (vt0, vt1), defining nonlinear transformation</li> </ul> <p>Parameter structure example: <pre><code>{\n  \"vt0\": \"velocity field x component\",\n  \"vt1\": \"velocity field y component\",\n  \"affineA\": \"3\u00d73 affine matrix\"\n}\n</code></pre></p> <p>Parameters can be saved via <code>output_params_path()</code> and loaded via <code>load_params_path()</code>, enabling transformation persistence and reuse.</p>"},{"location":"registration/registration-system-%28lddmm%29/#usage-examples","title":"Usage Examples","text":""},{"location":"registration/registration-system-%28lddmm%29/#basic-registration","title":"Basic Registration","text":"<ol> <li>Create <code>LDDMMRegistration</code> instance</li> <li>Load source and target images</li> <li>Execute registration</li> <li>Apply transformation or export parameters</li> </ol>"},{"location":"registration/registration-system-%28lddmm%29/#affine-registration-only","title":"Affine Registration Only","text":"<p>If only linear alignment is needed, call <code>run_affine()</code>.</p>"},{"location":"registration/registration-system-%28lddmm%29/#integration-with-spacemap","title":"Integration with SpaceMap","text":"<p>The LDDMM registration system is a core component of the SpaceMap alignment workflow, typically used after feature point coarse registration and before grid transformation. The final transformation is stored in <code>TransformDB</code> and can be used for subsequent processing of various data types.</p> <p>For reference source code and detailed implementation, please see the related source files above. </p>"},{"location":"workflow/key-workflows/","title":"Key Workflows","text":"<p>Related source files: - docs/assets/images/logo.png - docs/assets/images/qr.png - docs/examples.md - docs/index.md - docs/usage.md - flow/init.py - flow/afFlow2MultiDF.py - mkdocs.yml</p>"},{"location":"workflow/key-workflows/#introduction","title":"Introduction","text":"<p>This page outlines the main processing workflows for 3D tissue reconstruction in the SpaceMap framework, covering the entire process from raw data import to 3D model generation and spatial analysis.</p> <p>For detailed API, see API Reference, and for specific implementation cases, see Examples and Use Cases.</p>"},{"location":"workflow/key-workflows/#main-workflow-overview","title":"Main Workflow Overview","text":"<p>SpaceMap employs a multi-stage workflow, combining multi-scale feature matching with LDDMM (Large Deformation Diffeomorphic Metric Mapping) to balance efficiency and accuracy.</p> <p>Main Processing Workflow: 1. Data Import (FlowImport) 2. Image Registration (AutoFlowMultiCenter2/3) 3. 3D Reconstruction (TransformDB) 4. Spatial Analysis (FlowExport)</p>"},{"location":"workflow/key-workflows/#image-registration-workflow","title":"Image Registration Workflow","text":"<ul> <li>Coarse Registration: Affine registration + feature point matching (SIFT/LOFTR)</li> <li>Fine Registration: LDDMM nonlinear registration (ldm_pair), grid generation and merging</li> <li>Multi-center Registration Strategy: Using middle section as center, bidirectional registration to improve global consistency</li> </ul>"},{"location":"workflow/key-workflows/#3d-reconstruction-workflow","title":"3D Reconstruction Workflow","text":"<ul> <li>Grid Storage: Each registration step result saved as grid</li> <li>TransformDB unified management of all transformations</li> <li>Apply grid transformations to sections for point set/image 3D reconstruction</li> <li>Support grid merging to ensure global consistency</li> </ul>"},{"location":"workflow/key-workflows/#spatial-transcriptomics-analysis-workflow","title":"Spatial Transcriptomics Analysis Workflow","text":"<ul> <li>Cell-gene expression matrix analysis</li> <li>Gene expression spatial distribution analysis</li> <li>Cell type identification and spatial statistics</li> <li>3D visualization and export</li> </ul>"},{"location":"workflow/key-workflows/#end-to-end-integration-example","title":"End-to-End Integration Example","text":"<p>Typical complete workflow: 1. FlowImport imports data 2. AutoFlowMultiCenter2/3 performs registration 3. TransformDB manages and applies all transformations 4. FlowExport exports 3D model and analysis results</p>"},{"location":"workflow/key-workflows/#key-parameters-and-configuration","title":"Key Parameters and Configuration","text":"Parameter Description Default Component alignMethod Initial registration method \"auto\" AutoFlowMultiCenter2 gpu Computing GPU device None AutoFlowMultiCenter2 finalErr LDDMM convergence threshold System default ldm_pair centerTrain Center section training False ldm_pair show Show visualization results False Multiple locations saveGridKey Grid save key pairGridKey ldm_pair"},{"location":"workflow/key-workflows/#programming-and-development-guidelines","title":"Programming and Development Guidelines","text":"<ul> <li>Always initialize data with FlowImport</li> <li>Choose appropriate AutoFlow class (2/2DF/3)</li> <li>Execute affine, ldmm, grid merging in sequence</li> <li>Use TransformDB for unified transformation management</li> <li>Use FlowExport for 3D result export</li> </ul> <p>For reference source code and detailed implementation, please see the related source files above. </p>"}]}